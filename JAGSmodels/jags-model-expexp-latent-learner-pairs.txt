model{

  for(i in 1:N){
    rt[i] ~ dnorm(mu[i], 1 / sigma[subject_id[i]] ^ 2)
    
    mu[i] = alpha[subject_id[i], pair[i], is_predictable[i]+1] *  (1 + beta[subject_id[i]] * (exp(-gamma[subject_id[i]]*t[i]) -1)) * (1 - max(0,rlr[i,pair[i],is_predictable[i]+1]))
    
    for(p in 1:P[s]){
      rlr[i,p,1] = 0 
      rlr[i,p,2] = pair.learned[subject_id[i], pair[i]] * (beta.learn[subject_id[i],pair[i]] * (1 - exp(-gamma.learn[subject_id[i],pair[i]]*(t[i] - delta[subject_id[i],pair[i]]))))
    }
  }

  for(s in 1:S){
  
    # error in RT prediction at the subject level ###
    
    sigma[s] ~ dgamma(2, .01)
    
    # intercepts ####
    
    for(p in 1:P[s]){
      alpha[s,p,1] ~ dnorm(condition.alpha.mu[condition[s]], 1 / condition.alpha.sigma[condition[s]]^2)
      alpha[s,p,2] ~ dnorm(condition.alpha.mu[condition[s]], 1 / condition.alpha.sigma[condition[s]]^2)
    }
  
    # adaptation curves ###
    
    # beta (amount of adaptation)
    subject.adapt.beta[s] ~ dbeta(condition.adapt.beta.a[condition[s]], condition.adapt.beta.b[condition[s]])
    
    # gamma (rate of adaptation)
    subject.adapt.gamma[s] ~ dgamma(subject.adapt.gamma.sh[s], subject.adapt.gamma.ra[s])
    subject.adapt.gamma.sh[s] <- 1 + rate.mode[condition[s]] * subject.adapt.gamma.ra[condition[s]]
    subject.adapt.gamma.ra[s] <- (rate.mode[condition[s]] + sqrt( rate.mode[condition[s]]^2 + 4*rate.sd[condition[s]]^2)) / (2 * rate.sd[condition[s]]^2)
    
    # learning curves ###
    # item-level #
    # binary learning indicator for each pair
    # different beta, gamma, and delta values for each pair
  
    for(p in 1:P[s]){
      # binary learning indicator
      item.learned[s,p] ~ dbern(subject.prob.learn[s])
      
      # amount of learning for this pair (assuming it is learned)
      item.beta.learn[s,p] ~ dbeta(subject.beta.learn.a[s], subject.beta.learn.b[s])
      
      # rate of learning for this pair (assuming it is learned)
      item.gamma.learn[s,p] ~ dgamma(subject.gamma.learn.sh[s], subject.gamma.learn.ra[s])
      
      # onset of learning for this pair (assuming it is learned)
      item.delta.frac[s,p] ~ dbeta(subject.delta.learn.a[s], subject.delta.learn.b[s])
      item.delta[s,p] <- item.delta.frac[s,p] * max_t[s,p]
    }
    
    # subject-level learning curves #
    # hierarchical relationship, 
    # item-level learning curves drawn from subject level distribution.
    
    # probability of items showing learning for this subject
    subject.prob.learn[s] ~ dbeta(condition.prob.learn.a[condition[s]], condition.prob.learn.b[condition[s]])
    
    # amount of learning for items for this subject
    subject.beta.learn.a[s] <- subject.beta.learn.mode[s] * (subject.beta.learn.concentration[s] - 2) + 1
    subject.beta.learn.b[s] <- (1 - subject.beta.learn.mode[s]) * (subject.beta.learn.concentration[s] - 2) + 1
    subject.beta.learn.mode[s] ~ dbeta(condition.beta.learn.mode.a[condition[s]], condition.beta.learn.mode.b[condition[s]])
    subject.beta.learn.concentration[s] <- subject.beta.learn.concentration.k[s] + 2
    subject.beta.learn.concentration.k[s] ~ dgamma(condition.beta.learn.concentration.sh[condition[s]], condition.beta.learn.concentration.ra[condition[s]])
    
    # rate of learning for items for this subject
    subject.gamma.learn.mode[s] ~ dgamma(condition.gamma.learn.mode.sh[s], condition.gamma.learn.mode.ra[s])
    subject.gamma.learn.sd[s] ~ dgamma(condition.gamma.learn.sd.sh[s], condition.gamma.learn.sd.ra[s])
    subject.gamma.learn.sh[s] <- 1 + rate.learn.mode[condition[s]] * gamma.learn.ra[condition[s]]
    subject.gamma.learn.ra[s] <- (rate.learn.mode[condition[s]] + sqrt( rate.learn.mode[condition[s]]^2 + 4*rate.learn.sd[condition[s]]^2)) / (2 * rate.learn.sd[condition[s]]^2)
    
    # onset of learning for items for this subject
    subject.delta.learn.a[s] <- subject.delta.learn.mode[s] * (subject.delta.learn.concentration[s] - 2) +
    subject.delta.learn.b[s] <- (1 - subject.delta.learn.mode[s]) * (subject.delta.learn.concentration[s] - 2) +
    subject.delta.learn.mode[s] ~ dbeta(condition.delta.learn.mode.a[condition[s]], condition.delta.learn.mode.b[condition[s]])
    subject.delta.learn.concentration[s] <- subject.delta.learn.concentration.k[s] + 2
    subject.delta.learn.concentration.k[s] ~ dgamma(condition.delta.learn.concentration.sh[condition[s]], condition.delta.learn.concentration.ra[condition[s]])
    
  }

  for(c in 1:C){
  
    # intercepts ####
    
    condition.alpha.mu[c] ~ dnorm(1000, 1 / 500^2)T(0,2000) # weak prior
    condition.alpha.sigma[c] ~ dgamma(2, .01)
    
    # adaptation ####
    
    # beta adapt (distribution of subject beta values)
    
    condition.adapt.beta.a[c] <- condition.adapt.beta.mode[c] * (condition.adapt.beta.concentration[c] - 2) + 1
    condition.adapt.beta.b[c] <- (1 - condition.adapt.beta.mode[c]) * (condition.adapt.beta.concentration[c] - 2) + 1
    condition.adapt.beta.mode[c] ~ dbeta( , )
    condition.adapt.beta.concentration[c] <- condition.adapt.beta.concentration.k[c] + 2
    condition.adapt.beta.concentration.k[c] ~ dgamma( , )
    
    # gamma adapt (distribution of subject gamma values)
    
    condition.adapt.gamma.mode[c] ~ dexp(1)
    condition.adapt.gamma.sd[c] ~ dgamma(1,1)
    
    
    # learning ####
  
    # prob.learn (distribution of subject learning probabilities in each condition)
    
    condition.prob.learn.a[c] <- condition.prob.learn.mode[c] * (condition.prob.learn.concentration[c] - 2) + 1
    condition.prob.learn.b[c] <- (1 - condition.prob.learn.mode[c]) * (condition.prob.learn.concentration[c] - 2) + 1
    condition.prob.learn.mode[c] ~ dbeta(1,2) # weak preference for low values
    condition.prob.learn.concentration[c] <- condition.prob.learn.concentration.k[c] + 2
    condition.prob.learn.concentration.k[c] ~ dgamma ( , ) 
  
    
    # beta learn
    
    # gamma learn
    
    # delta learn
    
  }

}