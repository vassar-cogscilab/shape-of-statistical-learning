model{

  for(i in 1:N){
    rt[i] ~ dnorm(mu[i], 1 / sigma[subject_id[i]] ^ 2)
    
    mu[i] = alpha[subject_id[i], pair[i], is_predictable[i]+1] *  (1 + beta[subject_id[i]] * (exp(-gamma[subject_id[i]]*t[i]) -1)) * (1 - max(0,rlr[i,pair[i],is_predictable[i]+1]))
    
    for(p in 1:P){
      rlr[i,p,1] = 0 
      rlr[i,p,2] = pair.learned[subject_id[i], pair[i]] * (beta.learn[subject_id[i],pair[i]] * (1 - exp(-gamma.learn[subject_id[i],pair[i]]*(t[i] - delta[subject_id[i],pair[i]]))))
    }
  }

  for(s in 1:S){
  
    # error in RT prediction at the subject level ###
    
    sigma[s] ~ dgamma(2, .01)
    
    # intercepts ####
    
    for(p in 1:P[s]){
      alpha[s,p,1] ~ dnorm(alpha_mu[condition[s]], 1 / alpha_sigma[condition[s]]^2)
      alpha[s,p,2] ~ dnorm(alpha_mu[condition[s]], 1 / alpha_sigma[condition[s]]^2)
    }
  
    # adaptation curves ###
    # same beta and gamma values for all adaptation
    
    adapt.beta[s] ~ dbeta(beta_a[condition[s]], beta_b[condition[s]])
    
    adapt.gamma[s] ~ dgamma(adapt.gamma.sh[s], adapt.gamma.ra[s])
    adapt.gamma.sh[s] <- 1 + rate.mode[condition[s]] * adapt.gamma.ra[condition[s]]
    adapt.gamma.ra[s] <- (rate.mode[condition[s]] + sqrt( rate.mode[condition[s]]^2 + 4*rate.sd[condition[s]]^2)) / (2 * rate.sd[condition[s]]^2)
    
    # learning curves ###
    # item-level #
    # binary learning indicator for each pair
    # different beta, gamma, and delta values for each pair
  
    for(p in 1:P[s]){
      # binary learning indicator
      item.learned[s,p] ~ dbern(subject.prob.learn[s])
      
      # amount of learning for this pair (assuming it is learned)
      item.beta.learn[s,p] ~ dbeta(subject.beta.learn.a[s], subject.beta.learn.b[s])
      
      # rate of learning for this pair (assuming it is learned)
      item.gamma.learn[s,p] ~ dgamma(subject.gamma.learn.sh[s], subject.gamma.learn.ra[s])
      
      # onset of learning for this pair (assuming it is learned)
      item.delta.frac[s,p] ~ dbeta(subject.delta.learn.a[s], subject.delta.learn.b[s])
      item.delta[s,p] <- item.delta.frac[s,p] * max_t[s,p]
    }
    
    # subject-level learning curves #
    # hierarchical relationship, 
    # item-level learning curves drawn from subject level distribution.
    
    # probability of items showing learning for this subject
    subject.prob.learn[s] ~ dbeta(condition.prob.learn.a[condition[s]], condition.prob.learn.b[condition[s]])
    
    # amount of learning for items for this subject
    subject.beta.learn.a[s] <- subject.beta.learn.mode[s] * (subject.beta.learn.concentration[s] - 2) + 1
    subject.beta.learn.b[s] <- (1 - subject.beta.learn.mode[s]) * (subject.beta.learn.concentration[s] - 2) + 1
    subject.beta.learn.mode[s] ~ dbeta(condition.beta.learn.mode.a[condition[s]], condition.beta.learn.mode.b[condition[s]])
    subject.beta.learn.concentration[s] <- subject.beta.learn.concentration.k[s] + 2
    subject.beta.learn.concentration.k[s] ~ dgamma(condition.beta.learn.concentration.sh[condition[s]], condition.beta.learn.concentration.ra[condition[s]])
    
    # rate of learning for items for this subject
    subject.gamma.learn.mode[s] ~ dgamma(condition.gamma.learn.mode.sh[s], condition.gamma.learn.mode.ra[s])
    subject.gamma.learn.sd[s] ~ dgamma(condition.gamma.learn.sd.sh[s], condition.gamma.learn.sd.ra[s])
    subject.gamma.learn.sh[s] <- 1 + rate.learn.mode[condition[s]] * gamma.learn.ra[condition[s]]
    subject.gamma.learn.ra[s] <- (rate.learn.mode[condition[s]] + sqrt( rate.learn.mode[condition[s]]^2 + 4*rate.learn.sd[condition[s]]^2)) / (2 * rate.learn.sd[condition[s]]^2)
    
    # onset of learning for items for this subject
    subject.delta.learn.a[s] <- subject.delta.learn.mode[s] * (subject.delta.learn.concentration[s] - 2) +
    subject.delta.learn.b[s] <- (1 - subject.delta.learn.mode[s]) * (subject.delta.learn.concentration[s] - 2) +
    subject.delta.learn.mode[s] ~ dbeta(condition.delta.learn.mode.a[condition[s]], condition.delta.learn.mode.b[condition[s]])
    subject.delta.learn.concentration[s] <- subject.delta.learn.concentration.k[s] + 2
    subject.delta.learn.concentration.k[s] ~ dgamma(condition.delta.learn.concentration.sh[condition[s]], condition.delta.learn.concentration.ra[condition[s]])
    
  }

  for(c in 1:C){
    condition.prob.learn.a[c] <- condition.prob.learn.mode[c] * (condition.prob.learn.concentration[c] - 2) + 1
    condition.prob.learn.b[c] <- (1 - condition.prob.learn.mode[c]) * (condition.prob.learn.concentration[c] - 2) + 1
    condition.prob.learn.mode[c] ~ dbeta(1,2) # weak preference for low values
    condition.prob.learn.concentration[c] <- condition.prob.learn.concentration.k[c] + 2
    condition.prob.learn.concentration.k[c] ~ dgamma ( , ) 
  
    alpha_mu[c] ~ dunif(0, 2000)
    alpha_sigma[c] ~ dgamma(2, .01)
    
    rate.mode[c] ~ dexp(1)
    rate.sd[c] ~ dgamma(1,1)
    
    rate.learn.mode[c] ~ dexp(1)
    rate.learn.sd[c] ~ dgamma(1,1)
    
    beta.mode[c] ~ dbeta(2,2) # weak preference for middle values
    beta.concentration[c] <- beta.concentration.k[c] + 2
    beta.concentration.k[c] ~ dgamma(1.283196, 0.05663911) # mode = 5, sd = 20
  
    beta_a[c] <- beta.mode[c] * (beta.concentration[c]-2) + 1
    beta_b[c] <- (1 - beta.mode[c]) * (beta.concentration[c]-2) + 1
    
    beta.learn.mode[c] ~ dbeta(2,2) # weak preference for middle values
    beta.learn.concentration[c] <- beta.concentration.k[c] + 2
    beta.learn.concentration.k[c] ~ dgamma(1.283196, 0.05663911) # mode = 5, sd = 20
  
    beta.learn_a[c] <- beta.learn.mode[c] * (beta.learn.concentration[c]-2) + 1
    beta.learn_b[c] <- (1 - beta.learn.mode[c]) * (beta.learn.concentration[c]-2) + 1
    
    delta.mode[c] ~ dbeta(1,2) # weak preference for low values
    delta.concentration[c] <- delta.concentration.k[c] + 2
    delta.concentration.k[c] ~ dgamma(1.283196, 0.05663911) # mode = 5, sd = 20
  
    delta_a[c] <- delta.mode[c] * (delta.concentration[c]-2) + 1
    delta_b[c] <- (1 - delta.mode[c]) * (delta.concentration[c]-2) + 1
  }

}