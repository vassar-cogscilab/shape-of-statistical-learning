<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Vassar CogSci Lab Team" />

<meta name="date" content="2020-07-25" />

<title>Model Definitions and Analysis</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<script>// Hide empty <a> tag within highlighted CodeBlock for screen reader accessibility (see https://github.com/jgm/pandoc/issues/6352#issuecomment-626106786) -->
// v0.0.1
// Written by JooYoung Seo (jooyoung@psu.edu) and Atsushi Yasumoto on June 1st, 2020.

document.addEventListener('DOMContentLoaded', function() {
  const codeList = document.getElementsByClassName("sourceCode");
  for (var i = 0; i < codeList.length; i++) {
    var linkList = codeList[i].getElementsByTagName('a');
    for (var j = 0; j < linkList.length; j++) {
      if (linkList[j].innerHTML === "") {
        linkList[j].setAttribute('aria-hidden', 'true');
      }
    }
  }
});
</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>



<style type="text/css">
body {
background-color: #ffffff;
max-width: 800px;
margin-left: auto;
margin-right: auto;
margin-top: 0px;
margin-bottom: 0px;
padding-left: 5%;
padding-right: 4%;
padding-top: 10px;
padding-bottom: 30px;
overflow: visible;
font-family: Verdana;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
max-width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: .25em;
margin-bottom: .25em;
}
#TOC ul ul {
margin-left: -2em;
margin-top: 0px;
margin-bottom: 6px;
}
#TOC li {
list-style: disk outside;
line-height: 1.4;
margin-bottom: 5px;
}
#TOC li li {
list-style: circle outside;
line-height: 1.2;
margin-bottom: 0px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
p.method { background-color: #fcfcfc;
margin-left: 2em;
margin-right: 2em;
border-style: double;
border-width: 4px;
border-color: #b3fffa;
border-radius: 5px;
padding: 0.25em 0.75em 0.25em 1em;
}
span.math {
font-size: 1em;
}
span.eqref{
font-size: 0.75em;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
hr.sec1 {
margin-top: -1.5em;
border-width: 2px;
border-color: #aaaaaa;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: disk outside;
}
ul ul {
margin-bottom: 0;
}
ul ul li {
list-style: circle outside;
}
pre, code {
background-color: #f0f0f0;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 95%;
}
p > code, li > code {
padding: 2px 0px;
}
div.indent2 {
margin-left: 2%;
}
div.indent3 {
margin-left: 3%;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 175%;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #dbdbdb;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #e8e8e8;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Model Definitions and Analysis</h1>
<h4 class="author">Vassar CogSci Lab Team</h4>
<h4 class="date">July 25, 2020</h4>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<div id="TOC">
<ul>
<li>
<a href="#exp">The Experiment (Data Collection)</a>
</li>
<li>
<a href="#def">Model Definitions</a>
<ul>
<li>
<a href="#def-no">No Learning</a>
</li>
<li>
<a href="#def-step">Step Learning</a>
</li>
<li>
<a href="#def-sym">Symmetric Logistic Learning</a>
</li>
<li>
<a href="#def-asym">Asymmetric Logistic Learning</a>
</li>
</ul>
</li>
<li>
<a href="#rec">Model Recovery</a>
<ul>
<li>
<a href="#rec-nolearn">No Evidence of Learning</a>
</li>
<li>
<a href="#rec-learn">Strong Evidence of Learning</a>
</li>
<li>
<a href="#rec-small">Consistent but Small Decrease in Response Times</a>
</li>
<li>
<a href="#rec-ret">Sporadic Return to Baseline Response Time</a>
</li>
<li>
<a href="#rec-inc">Increased Response Times at the End of the Experiment</a>
</li>
</ul>
</li>
<li>
<a href="#comp">Formal Model Comparison (just a sample for now)</a>
<ul>
<li>
<a href="#comp-ppf">Posterior Predictive Fits</a>
</li>
<li>
<a href="#comp-bayes">Bayes Factor</a>
</li>
<li>
<a href="#comp-loo">Loo</a>
</li>
<li>
<a href="#fits-real">Stacking Weights</a>
</li>
</ul>
</li>
<li>
<a href="#references">References</a>
</li>
</ul>
</div>
<p>The Vassar CogSci Lab is modelling some pretty cool stuff. <br><br></p>
<div id="some-equation-with-a-paragraph" class="section level1">
<h1>Some equation with a paragraph</h1>
<p>This is just a sample so I know how to do it later.</p>
<p><span class="math display">\[\begin{equation} \label{eq:kl-Nav}
    k_\ell^{\text{Nav}} \left( t&#39;, \epsilon \right) = \left\lceil \max \left\{ \sqrt{\frac{-2 \log(\pi t&#39; \epsilon)}{\pi^2 t&#39;}}, \frac{1}{\pi \sqrt{t&#39;}} \right\} \right\rceil.
\tag{L.1}
\end{equation}\]</span></p>
<p>The small-time approximations, Equations <span class="math inline eqref"><span class="math inline">\(\eqref{eq:kl-Nav}\)</span></span> and</p>
</div>
<div id="exp" class="section level1">
<h1>The Experiment (Data Collection)</h1>
<hr class="sec1">
<p>This is my understanding of how the data were collected in “Experiment 1” from Josh’s thesis.</p>
<p>Participants were presented with a sequence of letters moving across window on a computer screen and were supposed to type the letter on the computer’s keyboard. The letters appeared animated and moved across the window from right to left, and they were only visible on the screen for two seconds. The response time for each typed letter was bounded below by zero seconds because the timing only started from the moment when the letter first started appearing on the right side of the window, well before the full letter was easily visible. The response times were also bounded above by two seconds because the letters were only visible on the screen for two seconds. In these sequences of letters, there were three different types of three-letter patterns that the participant could encounter. The patterns were sorted by difficulty and classified as one of: easy, three-letter words in the English language; medium, three-letter words in the English language but spelled backwards, or hard, a random permutation of a three-letter English word but was not itself a word in the English language. There was only one intentional pattern in the sequence of letters for each participant, and this pattern would randomly appear approximately seventy times in the sequence throughout the experiment. For more details on the experiment, see <span class="citation">Leeuw (2016)</span>.</p>
<p>Of particular interest to us are the sequences of response times for the first letter in the pattern and the third letter in the pattern. The idea is that since the first letter in the pattern is randomly placed in the sequence, the participant will exhibit a response time for the first letter that is consistent with their baseline response time for the rest of the random letters in the sequence. If the participant learns the pattern, we expect their response times for the first letter to remain consistent with their response times for the random letters in the sequence; however, we expect their response times for the subsequent letters (in particular the third letter) to decrease as they are able to anticipate this letter and begin to move their finger to the appropriate key before the letter appears on the screen. In the case that te participant does not learn the pattern among the sequence of letters, we expect their response times for every letter in the sequence to be consistent with their baseline response time for the random letters in the sequence. Our main objective with this model is to capture the <em>possible</em> decrease in response time for the third letter in the pattern, as this change indicates anticipatory behavior and recognition of the pattern.</p>
<p>Just a quick question about experiment design (because now I’m starting to think about it)- if there were experiment that follows the same setup to this one, but after showing the pattern many times (ideally after the participant learned the pattern, wonder how we could tell if they had!) the third letter in the pattern was changed to something random? I don’t know if there’s any point in this, but if the response time for the changed letter is higher than that for the baseline random letter would that also demonstrate that the individual had learned the pattern? I’m kinda thinking in the context of a teacher assigning math problems for homework, so maybe it’s too different because the “response time” for answering a math problem is likely greater than two seconds. But if you could measure how long it takes a student to do a particular type of problem and give the student a sequence of problems that are all similar, wouldn’t we expect to see a decrease in “response time” when the student figures out how to do that type of problem? Then to test if they actually learned the best strategy for solving that type of problem (instead of recognizing the pattern of what to do), the student could then be presented with a slightly different problem that requires a different strategy (perhaps something they haven’t learned yet). If the student just recognized the pattern of the previous problems instead of actually learning the material, then I would expect their response time to be consistent with their decreased learned response times (and their answer to be incorrect). On the other hand, if the student has worked out the strategy for the previous type of problem and understands the material, then I would think that their response time should be much greater because they are trying to figure out why their previous strategy doesn’t work. Maybe this would be totally unfeasible to ask kids to do a ton of math problems even though I’d probably enjoy it, and maybe it doesn’t work for a number of other reasons, but I thought I’d ask about it anyway.</p>
</div>
<div id="def" class="section level1">
<h1>The Models</h1>
<hr class="sec1">
<p>The starting point of these models is to define how the response times are distributed. As discussed by <span class="citation">Dablander (2019)</span>, we treat each response time from each subject to be Log-Normally distributed with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>; we’ll write it as <span class="math inline">\(rt \sim \mathcal{LN}(\mu, \sigma)\)</span>. The Log-Normal distribution has some useful properties for our application and we’ll take a quick look at some of them, but for extended information on the Log-Normal distribution see <a href="https://en.wikipedia.org/wiki/Log-normal_distribution">its Wikipedia page</a>. Primarily, the Log-Normal distribution has no support for negative response times, yet it resembles something of a normal distribution; these properties are consistent with the experimental data.</p>
<p>The mean, mode, and variance of the distribution are all slightly messy expressions involving both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, but the median of the distribution is a very clean <span class="math inline">\(\exp(\mu)\)</span>. In each of the expressions for these basic properties, the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> appear exponentiated. To exploit this structure and make our parameterization more convenient, we instead use the transformed parameters <span class="math inline">\(\hat{\mu} = \log(\mu)\)</span> and <span class="math inline">\(\hat{\sigma^2} = \log(\sigma^2)\)</span>. This transformation allows us to easily eliminate the exponentiation in the properties, and in particular it allows us to work directly with the median: <span class="math display">\[\begin{equation} \label{eqn:log-hat}
  \begin{aligned}
    \text{med}\big(rt \sim \mathcal{LN}(\hat{\mu}, \hat{\sigma^2})\big) &amp;= \exp\left[ \hat{\mu} \right] = \mu,\\[3ex]
    \text{mean}\big(rt \sim \mathcal{LN}(\hat{\mu}, \hat{\sigma^2})\big) &amp;= \exp\left[ \hat{\mu} + \frac{1}{2} \hat{\sigma^2} \right] = \mu \sigma,\\
    \text{mode}\big(rt \sim \mathcal{LN}(\hat{\mu}, \hat{\sigma^2})\big) &amp;= \exp\left[ \hat{\mu} - \hat{\sigma^2} \right] = \frac{\mu}{\sigma^2},\\
    \text{var}\big(rt \sim \mathcal{LN}(\hat{\mu}, \hat{\sigma^2})\big) &amp;= \left( \exp\left[ \hat{\sigma^2} \right] - 1 \right) \exp\left[ 2 \hat{\mu} + \hat{\sigma^2} \right] = \left( \sigma^2 - 1 \right) \mu^2 \sigma^2.
  \end{aligned}
  \nonumber
\end{equation}\]</span></p>
<p>Since we will use the transformed parameter <span class="math inline">\(\hat{\mu}\)</span>, our characterization of <span class="math inline">\(\mu\)</span> will equal the median of the distribution. We’ll build the model around this fact; when we define a functional form for <span class="math inline">\(\mu\)</span>, we’re actually defining the behavior of the median of the response time distribution. The main goal of the model is to capture a <em>possible</em> consistent decrease in response time after a specific trial, or in other words to identify a decrease in the median response time. This is why we’re going to work with <span class="math inline">\(\hat{\mu}\)</span> – so that we can directly change the median of the response time distribution.</p>
<p>A (potentially beneficial) side effect of working with <span class="math inline">\(\hat{\mu}\)</span> is that a decrease in <span class="math inline">\(\hat{\mu}\)</span> causes not only a decrease in the median response time, but it also causes a decrease in the variance of the response times (see the bottom equation in the above block). Josh and I had discussed this feature of the data a long time ago, but it seems to be logical that the response times are less variant once the participant learns the pattern. I haven’t placed much emphasis on <span class="math inline">\(\sigma^2\)</span> because it mainly controls the variance of the response time distribution, so it just gets fit with the data.</p>
<p>Each individual has response time data for both non-learned letters (denoted as <span class="math inline">\(rt_n\)</span>) and learned letters (denoted <span class="math inline">\(rt_\ell\)</span>). The response times of the non-learned letters should be consistent with the baseline response time for a random letter as the start of the pattern is randomly placed in the sequence. In contrast, the response times of the learned letters may be consistently shorter than the baseline as the participant may be able to anticipate the letters that appear later in the pattern. Since we could potentially have two distinct clusters of response times, we will fit these two clusters of response times separately; this yields fitting two distinct <span class="math inline">\(\hat{\mu}\)</span> (i.e., <span class="math inline">\(\hat{\mu_n}\)</span> and <span class="math inline">\(\hat{\mu_\ell}\)</span>), but we will use the same <span class="math inline">\(\hat{\sigma^2}\)</span> for each cluster. For each individual, the response time data will be modeled as <span class="math display">\[\begin{equation} \label{eqn:models}
  \begin{aligned}
    rt_n &amp;\sim \mathcal{LN}(\hat{\mu_n}, \hat{\sigma^2}),\\[2ex]
    rt_\ell &amp;\sim \mathcal{LN}(\hat{\mu_\ell}, \hat{\sigma^2}).\\[2ex]
  \end{aligned}
  \tag{M.0}
\end{equation}\]</span></p>
<p>The following subsections will define functional forms for <span class="math inline">\(\hat{\mu_n}\)</span> and <span class="math inline">\(\hat{\mu_\ell}\)</span> that enable the model to capture the evidence of learning (or lack of evidence) shown by each participant in the study. We will start with the simplest model and gradually add more features. Since the models will successively build on each other, the same parameters will arise in multiple models; although repetitive, we include the definitions of all parameters for completeness, but note that the old parameters will never be redefined. As we introduce each model, we will load the pre-compiled Stan model into the <code>R</code> workspace.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># load rstan and set options</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="kw">library</span>(<span class="st">&quot;rstan&quot;</span>)</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co">#&gt; Loading required package: StanHeaders</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co">#&gt; Loading required package: ggplot2</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="co">#&gt; rstan (Version 2.19.3, GitRev: 2e1f913d3ca3)</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">#&gt; For execution on a local, multicore CPU with excess RAM we recommend calling</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">#&gt; options(mc.cores = parallel::detectCores()).</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">#&gt; To avoid recompilation of unchanged Stan programs, we recommend calling</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co">#&gt; rstan_options(auto_write = TRUE)</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co">#&gt; For improved execution time, we recommend calling</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">#&gt; Sys.setenv(LOCAL_CPPFLAGS = &#39;-march=corei7 -mtune=corei7&#39;)</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co">#&gt; although this causes Stan to throw an error on a few processors.</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="kw">options</span>(<span class="dt">mc.cores =</span> parallel<span class="op">::</span><span class="kw">detectCores</span>())</span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="kw">rstan_options</span>(<span class="dt">auto_write =</span> <span class="ot">TRUE</span>)</span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="kw">Sys.setenv</span>(<span class="dt">LOCAL_CPPFLAGS =</span> <span class="st">&#39;-march=corei7 -mtune=corei7&#39;</span>)</span>
<span id="cb1-16"><a href="#cb1-16"></a></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="co"># set variable for filepaths depending on the operating system</span></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="cf">if</span> (.Platform<span class="op">$</span>OS.type <span class="op">==</span><span class="st"> &quot;windows&quot;</span>) { os &lt;-<span class="st"> &quot;windows&quot;</span> } <span class="cf">else</span> { os &lt;-<span class="st"> &quot;linux&quot;</span> }</span></code></pre></div>
<p>Even though we will not read and compile the models from scratch here, the <code>R</code> code to do so is available in the raw RMarkdown file.</p>
<div class="indent2">
<div id="def-no" class="section level2">
<h2>No Learning Model</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">load</span>(<span class="kw">paste0</span>(<span class="st">&quot;stan-models/compiled/&quot;</span>, os, <span class="st">&quot;/no_learning_model.Rds&quot;</span>))</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="co">#&gt; Warning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co">#&gt; &#39;stan-models/compiled/windows/no_learning_model.Rds&#39;, probable reason &#39;No such</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="co">#&gt; file or directory&#39;</span></span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="co">#&gt; Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection</span></span></code></pre></div>
<p>Assuming no learning occurs, this model is designed to fit the initial adaptation to the experimental task. This is the most basic model that we will test, as it defines the functional form for <span class="math inline">\(\hat{\mu}\)</span> as only a decaying exponential curve with a few parameters for fitting the data. The model for a single subject is:</p>
<p><span class="math display">\[\begin{equation} \label{eqn:model-no}
  \begin{aligned}
    \hat{\mu_n} &amp;= \log\big( V + E \cdot \exp(-A \cdot t) \big),\\[2ex]
    \hat{\mu_\ell} &amp;= \log\big( (V + S) + E \cdot \exp(-A \cdot t) \big),\\[2ex]
  \end{aligned}
  \tag{M.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(t\)</span> is the trial iteration, <span class="math inline">\(t \in \{1, \dots, T\}\)</span>, and <span class="math inline">\(T\)</span> is the number of trials performed in the experiment by that individual. The parameters are explained in detail in the following list:</p>
<ul>
<li><span class="math inline">\(V \in [0, 2]\)</span> represents the median pre-learning response time in seconds; technically, it is the vertical shift of all response times. Naturally, it should not be negative because response times cannot be negative in this experiment. Furthermore, the maximum value is 2 seconds because each letter was only visible in the window for 2 seconds before the next letter appeared and the response time was cut off.</li>
<li><span class="math inline">\(E \in [0, \infty)\)</span> is the overall scale of the exponential used to model the period of adaptation to the task. It must be non-negative because it must be monotonically decreasing as the subject gets used to the experiment.</li>
<li><span class="math inline">\(A \in [0, \infty)\)</span> is the adaptation rate to the task. It again must be non-negative so that the exponential term decays instead of grows.</li>
<li><span class="math inline">\(S \in (-\infty, \infty)\)</span> is the shift in response time of the learned letters  relative to that of the non-learned letters. We expect this value to be close to zero since essentially controls for any mechanical differences in typing the different letters on the keyboard.</li>
<li><span class="math inline">\(\sigma^2 \in [0, \infty)\)</span> drives the variance in the response times more than <span class="math inline">\(\mu&#39;\)</span>, so we treat it as a proxy for the variance; naturally, it should be non-negative.</li>
</ul>
<p>The prior distributions for the model parameters are:</p>
<p><span class="math display">\[\begin{equation} \label{eqn:model-no-prior}
  \begin{aligned}
    V &amp;\sim \text{Gamma}(2.5, 2.5),\\[0.25ex]
    E &amp;\sim \text{Gamma}(2.5, 10),\\[0.25ex]
    A &amp;\sim \text{Gamma}(2.5, 10),\\[0.25ex]
    S &amp;\sim \text{Normal}(0, 0.5),\\[0.25ex]
    \sigma &amp;\sim \text{Gamma}(2, 10).
  \end{aligned}
  \nonumber
\end{equation}\]</span></p>
<p>You might notice that the Gamma prior on <span class="math inline">\(V\)</span> technically has support for any non-negative real number, yet the response times themselves are bounded above by 2 seconds. There aren’t any useful priors that are defined on only <span class="math inline">\([0, 2]\)</span>, but we could use a prior bounded on <span class="math inline">\([0, 1]\)</span> and scale it to support <span class="math inline">\([0, 2]\)</span> (such as a Beta distribution). I opted for the Gamma prior so that the model would be more easily generalizable for other data, but the Stan model defines an upper bound of 2 seconds when fitting <span class="math inline">\(V\)</span> so any fitted value of <span class="math inline">\(V\)</span> can be at most <span class="math inline">\(2\)</span>.</p>
<p>We provide a fairly sharp Normal prior on <span class="math inline">\(S\)</span> so that it remains close to zero as we do not expect any major difference in the mechanical differences in typing on the keyboard. The other priors are weakly informative distributions based on their domains; since the priors are relatively weak, we allow for the data to drive the fitted parameter values instead of our prior beliefs. The priors on <span class="math inline">\(E\)</span>, <span class="math inline">\(A\)</span>, and <span class="math inline">\(\sigma\)</span> are simple Gamma distributions with ample room for exploring the possible parameter values.</p>
</div>
<div id="def-step" class="section level2">
<h2>Step Learning Model</h2>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">load</span>(<span class="kw">paste0</span>(<span class="st">&quot;stan-models/compiled/&quot;</span>, os, <span class="st">&quot;/step_learning_model.Rds&quot;</span>))</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="co">#&gt; Warning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file</span></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co">#&gt; &#39;stan-models/compiled/windows/step_learning_model.Rds&#39;, probable reason &#39;No such</span></span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="co">#&gt; file or directory&#39;</span></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="co">#&gt; Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection</span></span></code></pre></div>
<p>This model builds on the previous decaying exponential by including a sudden drop in the median response time to allow for fitting the evidence of learning. We add two parameters: <span class="math inline">\(D\)</span>, the scale of the drop in median response time, and <span class="math inline">\(H\)</span> the onset of learning. The sudden drop in response time takes the shape of a step function, which assumes that the learning occurs across the timespan of a single trial in the experiment. Whereas this model fits well to simple learning tasks where the learning occurs nearly instantaneously, it is rather inflexible for a more general learning process. The model for a single subject is:</p>
<p><span class="math display">\[\begin{equation} \label{eqn:models-step}
  \begin{aligned}
    \hat{\mu_n} &amp;= \log\big( V + E \cdot \exp(-A \cdot t) \big),\\[2ex]
    \hat{\mu_\ell} &amp;= \log\big( (V + S) + E \cdot \exp(-A \cdot t) - ({\bf 1}_{t \geq H} \cdot D) \big),\\[2ex]
  \end{aligned}
  \tag{M.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(t\)</span> is the trial iteration, <span class="math inline">\(t \in \{1, \dots, T\}\)</span>, <span class="math inline">\(T\)</span> is the number of trials performed in the experiment by that individual, and <span class="math inline">\({\bf 1}_{t \geq H}\)</span> is the indicator function that resolves to <span class="math inline">\(1\)</span> if <span class="math inline">\(t \geq H\)</span> and <span class="math inline">\(0\)</span> if <span class="math inline">\(t &lt; H\)</span>. In the coding of the model, care is taken to isolate the parameters <span class="math inline">\(S\)</span> and <span class="math inline">\(D\)</span> so that they do not get confounded; more precisely, the parameter <span class="math inline">\(D\)</span> is only applicable after <span class="math inline">\(H\)</span> trials have been completed by the participant. All of the parameters are explained in detail in the following list:</p>
<ul>
<li><span class="math inline">\(V \in [0, 2]\)</span> represents the median pre-learning response time in seconds; technically, it is the vertical shift of all response times. Naturally, it should not be negative because response times cannot be negative in this experiment. Furthermore, the maximum value is 2 seconds because each letter was only visible in the window for 2 seconds before the next letter appeared and the response time was cut off.</li>
<li><span class="math inline">\(E \in [0, \infty)\)</span> is the overall scale of the exponential used to model the period of adaptation to the task. It must be non-negative because it must be monotonically decreasing as the subject gets used to the experiment.</li>
<li><span class="math inline">\(A \in [0, \infty)\)</span> is the adaptation rate to the task. It again must be non-negative so that the exponential term decays instead of grows.</li>
<li><span class="math inline">\(S \in (-\infty, \infty)\)</span> is the shift in response time of the learned letters  relative to that of the non-learned letters. We expect this value to be close to zero since essentially controls for any mechanical differences in typing the different letters on the keyboard.</li>
<li><span class="math inline">\(D \in [0, 1]\)</span> is the scale of the learning “drop.” It is the scaled difference between the median pre-learning response time and the median post-learning response time for the learned letter in the pattern. A value near <span class="math inline">\(0\)</span> indicates a no decrease in response time (a post-learning response time equal to that of the pre-learning); a value near <span class="math inline">\(1\)</span> indicates a full decrease in response time (a post-learning response time of <span class="math inline">\(0\)</span> seconds).</li>
<li><span class="math inline">\(H \in [0, T]\)</span>, is the onset of learning, where <span class="math inline">\(T\)</span> is the number of trials performed in the experiment by the individual. <span class="math inline">\(H\)</span> represents how many trials it takes for the subject to learn the existence of the pattern.</li>
<li><span class="math inline">\(\sigma^2 \in [0, \infty)\)</span> drives the variance in the response times more than <span class="math inline">\(\mu&#39;\)</span>, so we treat it as a proxy for the variance; naturally, it should be non-negative.</li>
</ul>
<p>The prior distributions for the model parameters are:</p>
<p><span class="math display">\[\begin{equation} \label{eqn:model-step-prior}
  \begin{aligned}
    V &amp;\sim \text{Gamma}(2.5, 2.5),\\[0.25ex]
    E &amp;\sim \text{Gamma}(2.5, 10),\\[0.25ex]
    A &amp;\sim \text{Gamma}(2.5, 10),\\[0.25ex]
    S &amp;\sim \text{Normal}(0, 0.5),\\[0.25ex]
    D &amp;\sim \text{Beta}(2.5, 2.5),\\[0.25ex]
    H &amp;\sim \text{Cauchy}(\tfrac{T}{2}, 25),\\[0.25ex]
    \sigma &amp;\sim \text{Gamma}(2, 10).
  \end{aligned}
  \nonumber
\end{equation}\]</span></p>
<p>In addition to the priors already covered in the previous section, we place a slightly informative Beta prior on <span class="math inline">\(D\)</span> in order to dissuade the model from fitting values too close to either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. Whereas values close to <span class="math inline">\(1\)</span> are extremely unlikely, values close to <span class="math inline">\(0\)</span> can confound a consistent drop in the median response time with random noise, thus not identifying the underlying cognitive process.</p>
<p>Moreover, the prior on <span class="math inline">\(H\)</span> is also slightly informative as we discourage the model from fitting the onset of learning at the very beginning or the very end of the experiment. In cases such as these, we cannot distinguish legitimate evidence of learning behavior from simple adaptation to the task or random noise at the end of the experiment. While the Cauchy prior does technically support any real number (including negative numbers and zero), the Stan model bounds <span class="math inline">\(H\)</span> below by <span class="math inline">\(0\)</span> and above by <span class="math inline">\(T\)</span>, the number of trials completed by the individual (so the upper bound is different for each participant). Ultimately, this Cauchy prior suggests that the learning occurs somewhere around halfway through the experiment, but it doesn’t prevent the model from fitting fast or slow learners.</p>
</div>
<div id="def-sym" class="section level2">
<h2>Symmetric Logistic Learning Model</h2>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">load</span>(<span class="kw">paste0</span>(<span class="st">&quot;stan-models/compiled/&quot;</span>, os, <span class="st">&quot;/symmetric_logistic_learning_model.Rds&quot;</span>))</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="co">#&gt; Warning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file</span></span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="co">#&gt; &#39;stan-models/compiled/windows/symmetric_logistic_learning_model.Rds&#39;, probable</span></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="co">#&gt; reason &#39;No such file or directory&#39;</span></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="co">#&gt; Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection</span></span></code></pre></div>
<p>This model adds another layer of complexity to the Step Learning Model because it allows the drop in the median response time to be gradual by including the additional parameter <span class="math inline">\(L\)</span>, the learning rate. This model can approach a step function by allowing the learning rate to increase to <span class="math inline">\(\infty\)</span> (i.e., assuming that the learning happens instantaneously), yet it is still able to characterize gradual learning by fitting the learning rate to a value less than <span class="math inline">\(1\)</span>. Again, the parameter <span class="math inline">\(L\)</span> does not affect the model until <span class="math inline">\(H\)</span> trials have been completed by the participant. The model for a single subject is:</p>
<p><span class="math display">\[\begin{equation} \label{eqn:models-sym}
  \begin{aligned}
    \hat{\mu_n} &amp;= \log\big( V + E \cdot \exp(-A \cdot t) \big),\\[2ex]
    \hat{\mu_\ell} &amp;= \log\bigg( (V + S) + E \cdot \exp(-A \cdot t) - \cdot \Big( 1 - \frac{D}{1+\exp(-L \cdot (t-H))} \Big) \bigg),\\[2ex]
  \end{aligned}
  \tag{M.3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(t\)</span> is the trial iteration, <span class="math inline">\(t \in \{1, \dots, T\}\)</span>, and <span class="math inline">\(T\)</span> is the number of trials performed in the experiment by that individual. All of the parameters are explained in detail in the following list:</p>
<ul>
<li><span class="math inline">\(V \in [0, 2]\)</span> represents the median pre-learning response time in seconds; technically, it is the vertical shift of all response times. Naturally, it should not be negative because response times cannot be negative in this experiment. Furthermore, the maximum value is 2 seconds because each letter was only visible in the window for 2 seconds before the next letter appeared and the response time was cut off.</li>
<li><span class="math inline">\(E \in [0, \infty)\)</span> is the overall scale of the exponential used to model the period of adaptation to the task. It must be non-negative because it must be monotonically decreasing as the subject gets used to the experiment.</li>
<li><span class="math inline">\(A \in [0, \infty)\)</span> is the adaptation rate to the task. It again must be non-negative so that the exponential term decays instead of grows.</li>
<li><span class="math inline">\(S \in (-\infty, \infty)\)</span> is the shift in response time of the learned letters  relative to that of the non-learned letters. We expect this value to be close to zero since essentially controls for any mechanical differences in typing the different letters on the keyboard.</li>
<li><span class="math inline">\(D \in [0, 1]\)</span> is the scale of the learning “drop.” It is the scaled difference between the median pre-learning response time and the median post-learning response time for the learned letter in the pattern. A value near <span class="math inline">\(0\)</span> indicates a no decrease in response time (a post-learning response time equal to that of the pre-learning); a value near <span class="math inline">\(1\)</span> indicates a full decrease in response time (a post-learning response time of <span class="math inline">\(0\)</span> seconds).</li>
<li><span class="math inline">\(L \in [0, \infty)\)</span> is the learning rate of the subject. A negative learning rate would cause the logistic part of the expression to increase rather than decrease. A learning rate approaching <span class="math inline">\(\infty\)</span> approximates a step function (i.e., learning occurs instantaneously).</li>
<li><span class="math inline">\(H \in [0, T]\)</span>, is the onset of learning, where <span class="math inline">\(T\)</span> is the number of trials performed in the experiment by the individual. <span class="math inline">\(H\)</span> represents how many trials it takes for the subject to learn the existence of the pattern.</li>
<li><span class="math inline">\(\sigma^2 \in [0, \infty)\)</span> drives the variance in the response times more than <span class="math inline">\(\mu&#39;\)</span>, so we treat it as a proxy for the variance; naturally, it should be non-negative.</li>
</ul>
<p>The prior distributions for the model parameters are:</p>
<p><span class="math display">\[\begin{equation} \label{eqn:model-sym-prior}
  \begin{aligned}
    V &amp;\sim \text{Gamma}(2.5, 2.5),\\[0.25ex]
    E &amp;\sim \text{Gamma}(2.5, 10),\\[0.25ex]
    A &amp;\sim \text{Gamma}(2.5, 10),\\[0.25ex]
    S &amp;\sim \text{Normal}(0, 0.5),\\[0.25ex]
    D &amp;\sim \text{Beta}(2.5, 2.5),\\[0.25ex]
    L &amp;\sim \text{Gamma}(1.5, 0.25),\\[0.25ex]
    H &amp;\sim \text{Cauchy}(\tfrac{T}{2}, 25),\\[0.25ex]
    \sigma &amp;\sim \text{Gamma}(2, 10).
  \end{aligned}
  \nonumber
\end{equation}\]</span></p>
<p>The prior distribution that we place on <span class="math inline">\(L\)</span> is weakly informative, and it allows for fitting relatively large values of <span class="math inline">\(L\)</span> (e.g., <span class="math inline">\(L &gt; 5\)</span>) that cause the model to closely mimic a step function. This prior also allows the model to fit a smaller value for <span class="math inline">\(L\)</span>, characterizing a more gradual learning process.</p>
</div>
<div id="def-asym" class="section level2">
<h2>Asymmetric Logistic Learning Model</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">load</span>(<span class="kw">paste0</span>(<span class="st">&quot;stan-models/compiled/&quot;</span>, os, <span class="st">&quot;/asymmetric_logistic_learning_model.Rds&quot;</span>))</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co">#&gt; Warning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co">#&gt; &#39;stan-models/compiled/windows/asymmetric_logistic_learning_model.Rds&#39;, probable</span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="co">#&gt; reason &#39;No such file or directory&#39;</span></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co">#&gt; Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection</span></span></code></pre></div>
<p>The final model is very similar to the previous Symmetric Logistic Learning Model, but it uses the generalized form of the logistic function to model the shape of the decrease in median response time. By adding three additional parameters (<span class="math inline">\(C\)</span>, <span class="math inline">\(Q\)</span>, and <span class="math inline">\(\nu\)</span>), this model allows the drop in the median response time to be not only gradual but also asymmetric. Again, these new parameters do not affect the model until <span class="math inline">\(H\)</span> trials have been completed by the participant. The model for a single subject is:</p>
<p><span class="math display">\[\begin{equation} \label{eqn:models-asym}
  \begin{aligned}
    \hat{\mu_n} &amp;= \log\big( V + E \cdot \exp(-A \cdot t) \big),\\[2ex]
    \hat{\mu_\ell} &amp;= \log\bigg( (V + S) + E \cdot \exp(-A \cdot t) - \cdot \Big( 1 - \frac{D}{1+\exp(-L \cdot (t-H))} \Big) \bigg),\\[2ex]
  \end{aligned}
  \tag{M.3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(t\)</span> is the trial iteration, <span class="math inline">\(t \in \{1, \dots, T\}\)</span>, and <span class="math inline">\(T\)</span> is the number of trials performed in the experiment by that individual. All of the parameters are explained in detail in the following list:</p>
<ul>
<li><span class="math inline">\(V \in [0, 2]\)</span> represents the median pre-learning response time in seconds; technically, it is the vertical shift of all response times. Naturally, it should not be negative because response times cannot be negative in this experiment. Furthermore, the maximum value is 2 seconds because each letter was only visible in the window for 2 seconds before the next letter appeared and the response time was cut off.</li>
<li><span class="math inline">\(E \in [0, \infty)\)</span> is the overall scale of the exponential used to model the period of adaptation to the task. It must be non-negative because it must be monotonically decreasing as the subject gets used to the experiment.</li>
<li><span class="math inline">\(A \in [0, \infty)\)</span> is the adaptation rate to the task. It again must be non-negative so that the exponential term decays instead of grows.</li>
<li><span class="math inline">\(S \in (-\infty, \infty)\)</span> is the shift in response time of the learned letters  relative to that of the non-learned letters. We expect this value to be close to zero since essentially controls for any mechanical differences in typing the different letters on the keyboard.</li>
<li><span class="math inline">\(D \in [0, 1]\)</span> is the scale of the consistent decrease in median response time upon learning the existence of the pattern. It is the scaled difference between the median pre-learning response time and the median post-learning response time for the learned letter in the pattern. A value near <span class="math inline">\(0\)</span> indicates a no decrease in response time (a post-learning response time equal to that of the pre-learning); a value near <span class="math inline">\(1\)</span> indicates a full decrease in response time (a post-learning response time of <span class="math inline">\(0\)</span> seconds).</li>
<li><span class="math inline">\(L \in [0, \infty)\)</span> is the learning rate of the subject. A negative learning rate would cause the logistic part of the expression to increase rather than decrease. A learning rate approaching <span class="math inline">\(\infty\)</span> approximates a step function (i.e., learning occurs instantaneously).</li>
<li><span class="math inline">\(H \in [0, T]\)</span>, is the onset of learning, where <span class="math inline">\(T\)</span> is the number of trials performed in the experiment by the individual. <span class="math inline">\(H\)</span> represents how many trials it takes for the subject to learn the existence of the pattern.</li>
<li><span class="math inline">\(C \in [0, \infty)\)</span> inversely affects the scale of the consistent decrease in median response time upon learning the existence of the pattern.</li>
<li><span class="math inline">\(Q \in [0, \infty)\)</span> affects the onset of learning.</li>
<li><span class="math inline">\(\nu \in [0, \infty)\)</span> affects near which asymptote the maximum learning occurs (i.e., if learning occurs faster earlier or later in the learning process); this affect is only visible if the learning rate, <span class="math inline">\(L\)</span>, is small. In addition, it also affects the scale of the consistent decrease in median response time upon learning the existence of the pattern.</li>
<li><span class="math inline">\(\sigma^2 \in [0, \infty)\)</span> drives the variance in the response times more than <span class="math inline">\(\mu&#39;\)</span>, so we treat it as a proxy for the variance; naturally, it should be non-negative.</li>
</ul>
<p>The prior distributions for the model parameters are:</p>
<p><span class="math display">\[\begin{equation} \label{eqn:model-asym-prior}
  \begin{aligned}
    V &amp;\sim \text{Gamma}(2.5, 2.5),\\[0.25ex]
    E &amp;\sim \text{Gamma}(2.5, 10),\\[0.25ex]
    A &amp;\sim \text{Gamma}(2.5, 10),\\[0.25ex]
    S &amp;\sim \text{Normal}(0, 0.5),\\[0.25ex]
    D &amp;\sim \text{Beta}(2.5, 2.5),\\[0.25ex]
    L &amp;\sim \text{Gamma}(1.5, 0.25),\\[0.25ex]
    H &amp;\sim \text{Cauchy}(\tfrac{T}{2}, 25),\\[0.25ex]
    C &amp;\sim \text{Log-Normal}(1, 1),\\[0.25ex]
    Q &amp;\sim \text{Log-Normal}(1, 1),\\[0.25ex]
    \nu &amp;\sim \text{Log-Normal}(1, 1),\\[0.25ex]
    \sigma &amp;\sim \text{Gamma}(2, 10).
  \end{aligned}
  \nonumber
\end{equation}\]</span></p>
<p>All three of the prior distributions that we place on the newly added parameters (<span class="math inline">\(C\)</span>, <span class="math inline">\(Q\)</span>, and <span class="math inline">\(\nu\)</span>) are slightly informative Log-Normal distributions centered at <span class="math inline">\(1\)</span>. We chose the Log-Normal distribution because it only has support for non-negative real numbers, precisely the domain of the three newly added parameters. Moreover, these distributions are centered on <span class="math inline">\(1\)</span> but still allow the model to fit more extreme values. Note that if each of these parameters is <span class="math inline">\(1\)</span> (<span class="math inline">\(C = Q = \nu = 1\)</span>), then we return to the simpler symmetric logistic learning model in the previous subsection.</p>
</div>
</div>
</div>
<div id="rec" class="section level1">
<h1>Model Recovery</h1>
<p>In this section we will generate data to demonstrate the various patterns that the models should identify upon fitting.</p>
<div class="indent2">
<div id="rec-nolearn" class="section level2">
<h2>No Evidence of Learning</h2>
<div class="indent3">
<div id="rec-nolearn-no" class="section level3">
<h3>No Learning Model</h3>
</div>
<div id="rec-nolearn-step" class="section level3">
<h3>Step Learning Model</h3>
</div>
<div id="rec-nolearn-sym" class="section level3">
<h3>Symmetric Logistic Learning Model</h3>
</div>
<div id="rec-nolearn-asym" class="section level3">
<h3>Asymmetric Logistic Learning Model</h3>
</div>
</div>
</div>
<div id="rec-learn" class="section level2">
<h2>Strong Evidence of Learning</h2>
<div class="indent3">
<div id="rec-learn-no" class="section level3">
<h3>No Learning Model</h3>
</div>
<div id="rec-learn-step" class="section level3">
<h3>Step Learning Model</h3>
</div>
<div id="rec-learn-sym" class="section level3">
<h3>Symmetric Logistic Learning Model</h3>
</div>
<div id="rec-learn-asym" class="section level3">
<h3>Asymmetric Logistic Learning Model</h3>
</div>
</div>
</div>
<div id="rec-small" class="section level2">
<h2>Consistent but Small Decrease in Response Times</h2>
<div class="indent3">
<div id="rec-small-no" class="section level3">
<h3>No Learning Model</h3>
</div>
<div id="rec-small-step" class="section level3">
<h3>Step Learning Model</h3>
</div>
<div id="rec-small-sym" class="section level3">
<h3>Symmetric Logistic Learning Model</h3>
</div>
<div id="rec-small-asym" class="section level3">
<h3>Asymmetric Logistic Learning Model</h3>
</div>
</div>
</div>
<div id="rec-ret" class="section level2">
<h2>Sporadic Return to Baseline Response Time</h2>
<div class="indent3">
<div id="rec-ret-no" class="section level3">
<h3>No Learning Model</h3>
</div>
<div id="rec-ret-step" class="section level3">
<h3>Step Learning Model</h3>
</div>
<div id="rec-ret-sym" class="section level3">
<h3>Symmetric Logistic Learning Model</h3>
</div>
<div id="rec-ret-asym" class="section level3">
<h3>Asymmetric Logistic Learning Model</h3>
</div>
</div>
</div>
<div id="rec-inc" class="section level2">
<h2>Increased Response Times at the End of the Experiment</h2>
<div class="indent3">
<div id="rec-inc-no" class="section level3">
<h3>No Learning Model</h3>
</div>
<div id="rec-inc-step" class="section level3">
<h3>Step Learning Model</h3>
</div>
<div id="rec-inc-sym" class="section level3">
<h3>Symmetric Logistic Learning Model</h3>
</div>
<div id="rec-inc-asym" class="section level3">
<h3>Asymmetric Logistic Learning Model</h3>
</div>
</div>
</div>
</div>
</div>
<div id="comp" class="section level1">
<h1>Formal Model Comparison (just a sample for now)</h1>
<div class="indent2">
<div id="comp-ppf" class="section level2">
<h2>Posterior Predictive Fits</h2>
</div>
<div id="comp-bayes" class="section level2">
<h2>Bayes Factor</h2>
</div>
<div id="comp-loo" class="section level2">
<h2>Loo</h2>
</div>
<div id="comp-weights" class="section level2">
<h2>Stacking Weights</h2>
</div>
</div>
</div>
<div id="fits" class="section level1">
<h1>Sample Fits</h1>
<hr class="sec1">
<p>These are sample fits using the model. We’ll show simulated data for parameter recovery and real data collected from the experiment.</p>
<div id="fits-sim" class="section level2">
<h2>Fitting Simulated Data</h2>
<p>Here we’re basically just checking to make sure that the model recovers the parameters from the simulated data.</p>
<p>First we read the model into R using the <code>stanc</code> function; then we compile the model using the <code>stan_model</code> function. You can use a shortcut function to accomplish all of this and more in one function call, but I prefer to do it sequentially because I find it easier to catch bugs and other errors. Disregard the warnings about <span class="math inline">\(H\)</span>- it’s just because of how Stan makes you define the variable upper bounds.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">library</span>(<span class="st">&quot;rstan&quot;</span>)</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="kw">options</span>(<span class="dt">mc.cores =</span> parallel<span class="op">::</span><span class="kw">detectCores</span>())</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="kw">rstan_options</span>(<span class="dt">auto_write =</span> <span class="ot">TRUE</span>)</span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>exp_log_model_stanc &lt;-<span class="st"> </span><span class="kw">stanc</span>(<span class="dt">file =</span> <span class="st">&quot;../exp_logistic_model.stan&quot;</span>,</span>
<span id="cb6-6"><a href="#cb6-6"></a>                             <span class="dt">model_name =</span> <span class="st">&quot;exponential_logistic_model&quot;</span>)</span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="co">#&gt; Warning in file(fname, &quot;rt&quot;): cannot open file &#39;../exp_logistic_model.stan&#39;: No</span></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="co">#&gt; such file or directory</span></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="co">#&gt; Error in file(fname, &quot;rt&quot;) : cannot open the connection</span></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="co">#&gt; Error in get_model_strcode(file, model_code): cannot open model file &quot;../exp_logistic_model.stan&quot;</span></span>
<span id="cb6-11"><a href="#cb6-11"></a>exp_log_model &lt;-<span class="st"> </span><span class="kw">stan_model</span>(<span class="dt">stanc_ret =</span> exp_log_model_stanc)</span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="co">#&gt; Error in stan_model(stanc_ret = exp_log_model_stanc): object &#39;exp_log_model_stanc&#39; not found</span></span></code></pre></div>
<p>We’ll define the functions that we’ll use to generate some simulated data.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1"></a>exp_log &lt;-<span class="st"> </span><span class="cf">function</span>(trial, V, E, A, D, L, H) {</span>
<span id="cb7-2"><a href="#cb7-2"></a>  <span class="kw">return</span>( (V <span class="op">+</span><span class="st"> </span>E<span class="op">*</span><span class="kw">exp</span>(<span class="op">-</span>A<span class="op">*</span>trial)) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>D<span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>L<span class="op">*</span>(trial<span class="op">-</span>H)))) )</span>
<span id="cb7-3"><a href="#cb7-3"></a>}</span>
<span id="cb7-4"><a href="#cb7-4"></a>sim_exp_log &lt;-<span class="st"> </span><span class="cf">function</span>(trial, V, E, A, D, L, H, var) {</span>
<span id="cb7-5"><a href="#cb7-5"></a>  sim &lt;-<span class="st"> </span><span class="kw">exp_log</span>(<span class="dt">trial =</span> trial, <span class="dt">E =</span> E, <span class="dt">A =</span> A, <span class="dt">V =</span> V, <span class="dt">D =</span> D, <span class="dt">L =</span> L, <span class="dt">H =</span> H) <span class="op">+</span></span>
<span id="cb7-6"><a href="#cb7-6"></a><span class="st">         </span><span class="kw">rnorm</span>(<span class="kw">length</span>(trial), <span class="dv">0</span>, var)</span>
<span id="cb7-7"><a href="#cb7-7"></a>  sim[sim <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>  <span class="kw">return</span>(sim)</span>
<span id="cb7-9"><a href="#cb7-9"></a>}</span></code></pre></div>
<p>Let’s pick some parameter values and generate some simulated data. We’re making three participants: the first one learns the pattern after 50 trials, the second one does not learn the pattern, and the third one shows a very small decrease in response time after 30 trials. The last participant is there just to see how the model handles a tricky situation. I have not included the observed behavior of demonstrating evidence of learning the pattern yet sporadically producing response times that are consistent with the pre-learning median.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>K &lt;-<span class="st"> </span><span class="dv">3</span> <span class="co"># number of individuals</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>N &lt;-<span class="st"> </span><span class="dv">100</span> <span class="co"># number of trials for each individual (same for all)</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>NTI &lt;-<span class="st"> </span><span class="kw">rep</span>(N, K) <span class="co"># so Stan can handle multiple individuals</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>NK &lt;-<span class="st"> </span>N<span class="op">*</span>K</span>
<span id="cb8-5"><a href="#cb8-5"></a>Y0 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, NK) <span class="co"># vector of non-learned response times</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>Y1 &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, NK) <span class="co"># vector of learned response times</span></span>
<span id="cb8-7"><a href="#cb8-7"></a>V &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">1.1</span>, <span class="fl">1.5</span>, <span class="fl">.75</span>)</span>
<span id="cb8-8"><a href="#cb8-8"></a>E &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">25</span>, <span class="fl">.25</span>, <span class="fl">.25</span>)</span>
<span id="cb8-9"><a href="#cb8-9"></a>A &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">25</span>, <span class="fl">.25</span>, <span class="fl">.25</span>)</span>
<span id="cb8-10"><a href="#cb8-10"></a>P &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb8-11"><a href="#cb8-11"></a>D &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">4</span>, <span class="dv">0</span>, <span class="fl">.1</span>)</span>
<span id="cb8-12"><a href="#cb8-12"></a>L &lt;-<span class="st"> </span><span class="kw">c</span>(.<span class="dv">4</span>, <span class="fl">.4</span>, <span class="fl">.4</span>)</span>
<span id="cb8-13"><a href="#cb8-13"></a>H &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">50</span>, <span class="dv">60</span>, <span class="dv">30</span>)</span>
<span id="cb8-14"><a href="#cb8-14"></a>st &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb8-15"><a href="#cb8-15"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</span>
<span id="cb8-16"><a href="#cb8-16"></a>  x &lt;-<span class="st"> </span><span class="kw">seq_len</span>(NTI[i])</span>
<span id="cb8-17"><a href="#cb8-17"></a>  Y0[(st<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(st<span class="op">+</span>NTI[i])] &lt;-<span class="st"> </span><span class="kw">sim_exp_log</span>(x, <span class="dt">V=</span>V[i], <span class="dt">E=</span>E[i], <span class="dt">A=</span>A[i], <span class="dt">D=</span><span class="dv">0</span>, <span class="dt">L=</span>L[i], <span class="dt">H=</span>H[i], <span class="dt">var=</span><span class="fl">0.15</span>) <span class="co"># generate non-learned response times</span></span>
<span id="cb8-18"><a href="#cb8-18"></a>  Y1[(st<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(st<span class="op">+</span>NTI[i])] &lt;-<span class="st"> </span><span class="kw">sim_exp_log</span>(x, <span class="dt">V=</span>V[i], <span class="dt">E=</span>E[i], <span class="dt">A=</span>A[i], <span class="dt">D=</span>D[i], <span class="dt">L=</span>L[i], <span class="dt">H=</span>H[i], <span class="dt">var=</span><span class="fl">0.15</span>) <span class="co"># generate learned response times</span></span>
<span id="cb8-19"><a href="#cb8-19"></a>  st =<span class="st"> </span>st <span class="op">+</span><span class="st"> </span>NTI[i]</span>
<span id="cb8-20"><a href="#cb8-20"></a>}</span></code></pre></div>
<p>Now we’ll actually fit our generated data with our model. This a quick and dirty fit. It’s only running one chain, not a ton of iterations, and not incredibly strict controls with regard to how closely it attempts to fit the data. It will most certainly throw a lot of warnings, but in my experience these quick fits are still quite good and take under 30 seconds per individual. I’m just going to use the summary of the fit so we can easily see how well the parameters are recovered. Note that if you want to modify the R code and only fit one individual, it’s important to input <code>NTI</code> with <code>as.array()</code> because Stan can’t handle vectors of length one so it’s got to be an array instead. If you input more than one individual, you don’t need the <code>as.array()</code>, but it doesn’t hurt. Once the fit has run, we’ll print some of the summary statistics of the fit so we can see the means and standard deviations of the parameter estimates.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a>sim_fit &lt;-<span class="st"> </span><span class="kw">summary</span>(<span class="kw">sampling</span>(</span>
<span id="cb9-2"><a href="#cb9-2"></a>  exp_log_model,</span>
<span id="cb9-3"><a href="#cb9-3"></a>  <span class="dt">data =</span> <span class="kw">list</span>(<span class="st">&#39;K&#39;</span>=K, <span class="st">&#39;NTI&#39;</span>=<span class="kw">as.array</span>(NTI), <span class="st">&#39;NK&#39;</span>=NK, <span class="st">&#39;Y0&#39;</span>=Y0, <span class="st">&#39;Y1&#39;</span>=Y1),</span>
<span id="cb9-4"><a href="#cb9-4"></a>  <span class="dt">refresh =</span> <span class="ot">FALSE</span>, <span class="dt">chains =</span> <span class="dv">1</span>, <span class="dt">iter =</span> <span class="dv">500</span>, <span class="dt">seed =</span> <span class="dv">2</span>,</span>
<span id="cb9-5"><a href="#cb9-5"></a>  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>, <span class="dt">max_treedepth =</span> <span class="dv">10</span>)</span>
<span id="cb9-6"><a href="#cb9-6"></a>))<span class="op">$</span>summary</span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="co">#&gt; Error in h(simpleError(msg, call)): error in evaluating the argument &#39;object&#39; in selecting a method for function &#39;summary&#39;: error in evaluating the argument &#39;object&#39; in selecting a method for function &#39;sampling&#39;: object &#39;exp_log_model&#39; not found</span></span>
<span id="cb9-8"><a href="#cb9-8"></a></span>
<span id="cb9-9"><a href="#cb9-9"></a>sim_fit[, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>)]</span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="co">#&gt; Error in eval(expr, envir, enclos): object &#39;sim_fit&#39; not found</span></span></code></pre></div>
<p>Printing the summary tells us about the parameter estimates, but I think a visualization could be more useful. We’ll plot the mean estimate for each parameter, surrounded by the 2.5% and 97.5% confidence intervals from the fitting. On top of that we’ll plot the true value that was input to the simulation.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a>plot_sim_fit_pars &lt;-<span class="st"> </span><span class="cf">function</span>(fits, K, V, E, A, P, D, L, H) {</span>
<span id="cb10-2"><a href="#cb10-2"></a>  pars &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> <span class="kw">seq_len</span>(K),</span>
<span id="cb10-3"><a href="#cb10-3"></a>                     <span class="dt">V =</span> V,</span>
<span id="cb10-4"><a href="#cb10-4"></a>                     <span class="dt">E =</span> E,</span>
<span id="cb10-5"><a href="#cb10-5"></a>                     <span class="dt">A =</span> A,</span>
<span id="cb10-6"><a href="#cb10-6"></a>                     <span class="dt">P =</span> P,</span>
<span id="cb10-7"><a href="#cb10-7"></a>                     <span class="dt">D =</span> D,</span>
<span id="cb10-8"><a href="#cb10-8"></a>                     <span class="dt">L =</span> L,</span>
<span id="cb10-9"><a href="#cb10-9"></a>                     <span class="dt">H =</span> H)</span>
<span id="cb10-10"><a href="#cb10-10"></a>  idx &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">6</span>, <span class="dv">9</span>)</span>
<span id="cb10-11"><a href="#cb10-11"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(<span class="kw">length</span>(idx))) {</span>
<span id="cb10-12"><a href="#cb10-12"></a>    df &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(fits[<span class="kw">seq_len</span>(K) <span class="op">+</span><span class="st"> </span>(idx[i]<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>K, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">8</span>)])</span>
<span id="cb10-13"><a href="#cb10-13"></a>    <span class="kw">colnames</span>(df) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mean&quot;</span>, <span class="st">&quot;qlow&quot;</span>, <span class="st">&quot;qupp&quot;</span>)</span>
<span id="cb10-14"><a href="#cb10-14"></a>    df<span class="op">$</span>id &lt;-<span class="st"> </span><span class="kw">seq_len</span>(K)</span>
<span id="cb10-15"><a href="#cb10-15"></a>    df<span class="op">$</span>dumm =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">2</span>, K<span class="dv">-1</span>))</span>
<span id="cb10-16"><a href="#cb10-16"></a>    par_name &lt;-<span class="st"> </span><span class="kw">substring</span>(<span class="kw">rownames</span>(df)[<span class="dv">1</span>], <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb10-17"><a href="#cb10-17"></a></span>
<span id="cb10-18"><a href="#cb10-18"></a>    <span class="kw">print</span>(<span class="kw">ggplot</span>(df) <span class="op">+</span></span>
<span id="cb10-19"><a href="#cb10-19"></a><span class="st">      </span><span class="kw">geom_pointrange</span>(<span class="dt">alpha =</span> <span class="fl">0.6</span>, <span class="dt">shape =</span> <span class="dv">16</span>, <span class="dt">color =</span> <span class="st">&quot;#5fc6fa&quot;</span>,</span>
<span id="cb10-20"><a href="#cb10-20"></a>                      <span class="kw">aes</span>(<span class="dt">x =</span> id, <span class="dt">y =</span> mean, <span class="dt">ymin =</span> qlow, <span class="dt">ymax =</span> qupp,</span>
<span id="cb10-21"><a href="#cb10-21"></a>                          <span class="dt">size =</span> <span class="kw">factor</span>(dumm, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)))) <span class="op">+</span></span>
<span id="cb10-22"><a href="#cb10-22"></a><span class="st">      </span><span class="kw">geom_point</span>(<span class="dt">data =</span> pars, <span class="dt">alpha =</span> <span class="fl">0.8</span>, <span class="dt">color =</span> <span class="st">&quot;#04547c&quot;</span>,</span>
<span id="cb10-23"><a href="#cb10-23"></a>                 <span class="dt">shape =</span> <span class="dv">4</span>, <span class="dt">size =</span> <span class="dv">6</span>, <span class="dt">stroke =</span> <span class="dv">2</span>,</span>
<span id="cb10-24"><a href="#cb10-24"></a>                 <span class="kw">aes_string</span>(<span class="dt">x =</span> <span class="st">&quot;id&quot;</span>, <span class="dt">y =</span> par_name)) <span class="op">+</span></span>
<span id="cb10-25"><a href="#cb10-25"></a><span class="st">      </span><span class="kw">scale_size_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="fl">1.51</span>, <span class="fl">1.5</span>),</span>
<span id="cb10-26"><a href="#cb10-26"></a>                        <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;True Value&quot;</span>, <span class="st">&quot;Est. Mean&quot;</span>),</span>
<span id="cb10-27"><a href="#cb10-27"></a>                        <span class="dt">name =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb10-28"><a href="#cb10-28"></a><span class="st">      </span><span class="kw">guides</span>(<span class="dt">size =</span> <span class="kw">guide_legend</span>(<span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">size =</span> <span class="kw">c</span>(<span class="fl">1.5</span>, <span class="fl">1.5</span>),</span>
<span id="cb10-29"><a href="#cb10-29"></a>                                                     <span class="dt">stroke =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb10-30"><a href="#cb10-30"></a>                                                     <span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">16</span>),</span>
<span id="cb10-31"><a href="#cb10-31"></a>                                                     <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>),</span>
<span id="cb10-32"><a href="#cb10-32"></a>                                                     <span class="dt">color =</span> <span class="kw">c</span>(<span class="st">&quot;#04547c&quot;</span>,</span>
<span id="cb10-33"><a href="#cb10-33"></a>                                                               <span class="st">&quot;#5fc6fa&quot;</span>)))) <span class="op">+</span></span>
<span id="cb10-34"><a href="#cb10-34"></a></span>
<span id="cb10-35"><a href="#cb10-35"></a><span class="st">      </span><span class="kw">scale_x_discrete</span>(<span class="dt">limits =</span> <span class="kw">as.character</span>(<span class="kw">seq_len</span>(K))) <span class="op">+</span></span>
<span id="cb10-36"><a href="#cb10-36"></a><span class="st">      </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste0</span>(<span class="st">&quot;Simulated Parameter Estimates for &quot;</span>, par_name),</span>
<span id="cb10-37"><a href="#cb10-37"></a>           <span class="dt">x =</span> <span class="st">&quot;ID of Individual&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Estimated Parameter Value&quot;</span>,</span>
<span id="cb10-38"><a href="#cb10-38"></a>           <span class="dt">subtitle =</span> <span class="st">&quot;Bars represent the 2.5% and 97.5% CI&quot;</span>) <span class="op">+</span></span>
<span id="cb10-39"><a href="#cb10-39"></a><span class="st">      </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb10-40"><a href="#cb10-40"></a><span class="st">      </span><span class="kw">theme</span>(<span class="dt">panel.border =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb10-41"><a href="#cb10-41"></a>            <span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">20</span>),</span>
<span id="cb10-42"><a href="#cb10-42"></a>            <span class="dt">plot.subtitle =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>,</span>
<span id="cb10-43"><a href="#cb10-43"></a>                                         <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">15</span>, <span class="dv">5</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb10-44"><a href="#cb10-44"></a>            <span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">14</span>),</span>
<span id="cb10-45"><a href="#cb10-45"></a>            <span class="dt">axis.text.y =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">14</span>),</span>
<span id="cb10-46"><a href="#cb10-46"></a>            <span class="dt">axis.title.x =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>,</span>
<span id="cb10-47"><a href="#cb10-47"></a>                                        <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb10-48"><a href="#cb10-48"></a>            <span class="dt">axis.title.y =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>,</span>
<span id="cb10-49"><a href="#cb10-49"></a>                                        <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb10-50"><a href="#cb10-50"></a>            <span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb10-51"><a href="#cb10-51"></a>            <span class="dt">legend.justification =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb10-52"><a href="#cb10-52"></a>            <span class="dt">legend.box =</span> <span class="st">&quot;horizontal&quot;</span>,</span>
<span id="cb10-53"><a href="#cb10-53"></a>            <span class="dt">legend.direction =</span> <span class="st">&quot;vertical&quot;</span>,</span>
<span id="cb10-54"><a href="#cb10-54"></a>            <span class="dt">legend.background =</span> <span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="st">&quot;transparent&quot;</span>))</span>
<span id="cb10-55"><a href="#cb10-55"></a>    )</span>
<span id="cb10-56"><a href="#cb10-56"></a>  }</span>
<span id="cb10-57"><a href="#cb10-57"></a>}</span>
<span id="cb10-58"><a href="#cb10-58"></a></span>
<span id="cb10-59"><a href="#cb10-59"></a><span class="kw">plot_sim_fit_pars</span>(sim_fit, K, V, E, A, P, D, L, H)</span>
<span id="cb10-60"><a href="#cb10-60"></a><span class="co">#&gt; Error in as.data.frame(fits[seq_len(K) + (idx[i] - 1) * K, c(1, 4, 8)]): object &#39;sim_fit&#39; not found</span></span></code></pre></div>
<p>As we suspected, some parameters are more difficult to fit than others. The parameters <span class="math inline">\(E\)</span> and <span class="math inline">\(A\)</span> are slightly loose, and I attribute this to a lack of data since the acclimation to the task should be very short. Still, the parameter estimates don’t seem wildly off. At first glance, it appears that the parameters <span class="math inline">\(D\)</span>, <span class="math inline">\(L\)</span>, and <span class="math inline">\(H\)</span> have poor fits for individuals 2 and 3. This lack of fitting is expected, though, because these parameters have no meaning when <span class="math inline">\(P\)</span> is close to zero as there is no learning to fit. Individual 3 does pose problems for the model, and we’ll see some more evidence of this in the next plot.</p>
<p>Now we’ll plot the fits over the generated data to give a visual as to how well the fitted parameters match the data. We’ll write a function to do the plotting so all of the temporary bits are nicely contained.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a>plot_sim_fits &lt;-<span class="st"> </span><span class="cf">function</span>(fits, K, NTI,</span>
<span id="cb11-2"><a href="#cb11-2"></a>                          <span class="dt">V =</span> <span class="ot">NA</span>, <span class="dt">E =</span> <span class="ot">NA</span>, <span class="dt">A =</span> <span class="ot">NA</span>, <span class="dt">D =</span> <span class="ot">NA</span>,</span>
<span id="cb11-3"><a href="#cb11-3"></a>                          <span class="dt">L =</span> <span class="ot">NA</span>, <span class="dt">H =</span> <span class="ot">NA</span>, <span class="dt">Y0 =</span> <span class="ot">NA</span>, <span class="dt">Y1 =</span> <span class="ot">NA</span>,</span>
<span id="cb11-4"><a href="#cb11-4"></a>                          <span class="dt">savepath =</span> <span class="ot">NULL</span>) {</span>
<span id="cb11-5"><a href="#cb11-5"></a>  st &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</span>
<span id="cb11-7"><a href="#cb11-7"></a>    n &lt;-<span class="st"> </span>NTI[i]</span>
<span id="cb11-8"><a href="#cb11-8"></a>    Vi &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">0</span><span class="op">*</span>K]</span>
<span id="cb11-9"><a href="#cb11-9"></a>    Ei &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">1</span><span class="op">*</span>K]</span>
<span id="cb11-10"><a href="#cb11-10"></a>    Ai &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">2</span><span class="op">*</span>K]</span>
<span id="cb11-11"><a href="#cb11-11"></a>    Pi &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">3</span><span class="op">*</span>K]</span>
<span id="cb11-12"><a href="#cb11-12"></a>    Di &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">4</span><span class="op">*</span>K]</span>
<span id="cb11-13"><a href="#cb11-13"></a>    Li &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">5</span><span class="op">*</span>K]</span>
<span id="cb11-14"><a href="#cb11-14"></a>    Hi &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">8</span><span class="op">*</span>K]</span>
<span id="cb11-15"><a href="#cb11-15"></a>    sigma_2i &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">7</span><span class="op">*</span>K]</span>
<span id="cb11-16"><a href="#cb11-16"></a>    sn &lt;-<span class="st"> </span><span class="kw">seq_len</span>(n)</span>
<span id="cb11-17"><a href="#cb11-17"></a></span>
<span id="cb11-18"><a href="#cb11-18"></a>    df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">rep</span>(sn, <span class="dv">2</span>),</span>
<span id="cb11-19"><a href="#cb11-19"></a>                     <span class="dt">data =</span> <span class="kw">c</span>(Y0[(st<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(st<span class="op">+</span>n)], Y1[(st<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(st<span class="op">+</span>n)]),</span>
<span id="cb11-20"><a href="#cb11-20"></a>                     <span class="dt">truth =</span> <span class="kw">c</span>(<span class="kw">exp_log</span>(sn, <span class="dt">V =</span> V[i], <span class="dt">E =</span> E[i], <span class="dt">A =</span> A[i],</span>
<span id="cb11-21"><a href="#cb11-21"></a>                                       <span class="dt">D =</span> <span class="dv">0</span>, <span class="dt">L =</span> L[i], <span class="dt">H =</span> H[i]),</span>
<span id="cb11-22"><a href="#cb11-22"></a>                               <span class="kw">exp_log</span>(sn, <span class="dt">V =</span> V[i], <span class="dt">E =</span> E[i], <span class="dt">A =</span> A[i],</span>
<span id="cb11-23"><a href="#cb11-23"></a>                                       <span class="dt">D =</span> D[i], <span class="dt">L =</span> L[i], <span class="dt">H =</span> H[i])),</span>
<span id="cb11-24"><a href="#cb11-24"></a>                     <span class="dt">fit =</span> <span class="kw">c</span>(<span class="kw">exp_log</span>(sn, <span class="dt">V =</span> Vi, <span class="dt">E =</span> Ei, <span class="dt">A =</span> Ai,</span>
<span id="cb11-25"><a href="#cb11-25"></a>                                     <span class="dt">D =</span> <span class="dv">0</span>, <span class="dt">L =</span> Li, <span class="dt">H =</span> Hi),</span>
<span id="cb11-26"><a href="#cb11-26"></a>                             <span class="kw">exp_log</span>(sn, <span class="dt">V =</span> Vi, <span class="dt">E =</span> Ei, <span class="dt">A =</span> Ai,</span>
<span id="cb11-27"><a href="#cb11-27"></a>                                     <span class="dt">D =</span> Pi <span class="op">*</span><span class="st"> </span>Di, <span class="dt">L =</span> Li, <span class="dt">H =</span> Hi)),</span>
<span id="cb11-28"><a href="#cb11-28"></a>                     <span class="dt">data_label =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n), <span class="kw">rep</span>(<span class="dv">2</span>,n)),</span>
<span id="cb11-29"><a href="#cb11-29"></a>                     <span class="dt">fit_label =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">3</span>,n), <span class="kw">rep</span>(<span class="dv">4</span>,n)))</span>
<span id="cb11-30"><a href="#cb11-30"></a>    df<span class="op">$</span>fit_min &lt;-<span class="st"> </span>df<span class="op">$</span>fit <span class="op">-</span><span class="st"> </span>df<span class="op">$</span>fit<span class="op">*</span><span class="kw">sqrt</span>(sigma_2i<span class="op">*</span>(sigma_2i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span>
<span id="cb11-31"><a href="#cb11-31"></a>    df<span class="op">$</span>fit_max &lt;-<span class="st"> </span>df<span class="op">$</span>fit <span class="op">+</span><span class="st"> </span>df<span class="op">$</span>fit<span class="op">*</span><span class="kw">sqrt</span>(sigma_2i<span class="op">*</span>(sigma_2i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span>
<span id="cb11-32"><a href="#cb11-32"></a>    df<span class="op">$</span>dumm &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), <span class="kw">rep</span>(<span class="dv">2</span>, n))</span>
<span id="cb11-33"><a href="#cb11-33"></a></span>
<span id="cb11-34"><a href="#cb11-34"></a>    <span class="co"># factor levels key (since R makes it alphabetical)</span></span>
<span id="cb11-35"><a href="#cb11-35"></a>      <span class="co"># 1: Non-Learned Data/Truth</span></span>
<span id="cb11-36"><a href="#cb11-36"></a>      <span class="co"># 2: Learned Data/Truth</span></span>
<span id="cb11-37"><a href="#cb11-37"></a>      <span class="co"># 3: Non-Learned Fit</span></span>
<span id="cb11-38"><a href="#cb11-38"></a>      <span class="co"># 4: Learned Fit</span></span>
<span id="cb11-39"><a href="#cb11-39"></a></span>
<span id="cb11-40"><a href="#cb11-40"></a>    <span class="kw">print</span>(<span class="kw">ggplot</span>(df) <span class="op">+</span></span>
<span id="cb11-41"><a href="#cb11-41"></a><span class="st">      </span><span class="kw">geom_ribbon</span>(<span class="dt">linetype=</span><span class="st">&quot;blank&quot;</span>, <span class="dt">alpha=</span><span class="fl">0.25</span>,</span>
<span id="cb11-42"><a href="#cb11-42"></a>                  <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">ymin =</span> fit_min, <span class="dt">ymax =</span> fit_max,</span>
<span id="cb11-43"><a href="#cb11-43"></a>                      <span class="dt">fill =</span> <span class="kw">factor</span>(fit_label, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>)))) <span class="op">+</span></span>
<span id="cb11-44"><a href="#cb11-44"></a><span class="st">      </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.75</span>, <span class="dt">shape =</span> <span class="dv">16</span>,</span>
<span id="cb11-45"><a href="#cb11-45"></a>                 <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> data,</span>
<span id="cb11-46"><a href="#cb11-46"></a>                     <span class="dt">color =</span> <span class="kw">factor</span>(data_label, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)),</span>
<span id="cb11-47"><a href="#cb11-47"></a>                     <span class="dt">size =</span> <span class="kw">factor</span>(dumm, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)))) <span class="op">+</span></span>
<span id="cb11-48"><a href="#cb11-48"></a><span class="st">      </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.25</span>, <span class="dt">alpha =</span> <span class="fl">0.8</span>,</span>
<span id="cb11-49"><a href="#cb11-49"></a>                <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> truth,</span>
<span id="cb11-50"><a href="#cb11-50"></a>                    <span class="dt">color =</span> <span class="kw">factor</span>(data_label, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)))) <span class="op">+</span></span>
<span id="cb11-51"><a href="#cb11-51"></a><span class="st">      </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.25</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>,</span>
<span id="cb11-52"><a href="#cb11-52"></a>                <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> fit,</span>
<span id="cb11-53"><a href="#cb11-53"></a>                    <span class="dt">color =</span> <span class="kw">factor</span>(fit_label, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>)))) <span class="op">+</span></span>
<span id="cb11-54"><a href="#cb11-54"></a><span class="st">      </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;#999999&quot;</span>, <span class="st">&quot;#000000&quot;</span>,</span>
<span id="cb11-55"><a href="#cb11-55"></a>                                    <span class="st">&quot;#98c1ff&quot;</span>, <span class="st">&quot;#ff4f4f&quot;</span>),</span>
<span id="cb11-56"><a href="#cb11-56"></a>                         <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Non-Learned Truth&quot;</span>, <span class="st">&quot;Learned Truth&quot;</span>,</span>
<span id="cb11-57"><a href="#cb11-57"></a>                                    <span class="st">&quot;Non-Learned Fit&quot;</span>, <span class="st">&quot;Learned Fit&quot;</span>),</span>
<span id="cb11-58"><a href="#cb11-58"></a>                         <span class="dt">name =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb11-59"><a href="#cb11-59"></a><span class="st">      </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;#98c1ff&quot;</span>, <span class="st">&quot;#ff4f4f&quot;</span>),</span>
<span id="cb11-60"><a href="#cb11-60"></a>                        <span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb11-61"><a href="#cb11-61"></a><span class="st">      </span><span class="kw">scale_size_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="fl">1.8</span>, <span class="fl">1.81</span>),</span>
<span id="cb11-62"><a href="#cb11-62"></a>                        <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Non-Learned Data&quot;</span>, <span class="st">&quot;Learned Data&quot;</span>),</span>
<span id="cb11-63"><a href="#cb11-63"></a>                        <span class="dt">name =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb11-64"><a href="#cb11-64"></a><span class="st">      </span><span class="kw">guides</span>(<span class="dt">color =</span> <span class="kw">guide_legend</span>(<span class="dt">order =</span> <span class="dv">2</span>,</span>
<span id="cb11-65"><a href="#cb11-65"></a>                                  <span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">shape =</span> <span class="ot">NA</span>,</span>
<span id="cb11-66"><a href="#cb11-66"></a>                                                      <span class="dt">size =</span> <span class="fl">1.25</span>,</span>
<span id="cb11-67"><a href="#cb11-67"></a>                                                      <span class="dt">lty =</span> <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">4</span>))),</span>
<span id="cb11-68"><a href="#cb11-68"></a>             <span class="dt">size =</span> <span class="kw">guide_legend</span>(<span class="dt">order =</span> <span class="dv">1</span>,</span>
<span id="cb11-69"><a href="#cb11-69"></a>                                 <span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">size =</span> <span class="dv">2</span>,</span>
<span id="cb11-70"><a href="#cb11-70"></a>                                                     <span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">16</span>),</span>
<span id="cb11-71"><a href="#cb11-71"></a>                                                     <span class="dt">color =</span> <span class="kw">c</span>(<span class="st">&quot;#999999&quot;</span>,</span>
<span id="cb11-72"><a href="#cb11-72"></a>                                                               <span class="st">&quot;#000000&quot;</span>)))) <span class="op">+</span></span>
<span id="cb11-73"><a href="#cb11-73"></a><span class="st">      </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Simulated Data Fit&quot;</span>,</span>
<span id="cb11-74"><a href="#cb11-74"></a>           <span class="dt">x =</span> <span class="st">&quot;Trial Number&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Response Time (sec)&quot;</span>,</span>
<span id="cb11-75"><a href="#cb11-75"></a>           <span class="dt">subtitle =</span> <span class="kw">paste0</span>(<span class="st">&quot;\u00B1 1 standard deviation</span><span class="ch">\n</span><span class="st">&quot;</span>,</span>
<span id="cb11-76"><a href="#cb11-76"></a>                             <span class="st">&quot;Subject &quot;</span>, i, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>,</span>
<span id="cb11-77"><a href="#cb11-77"></a>                             <span class="st">&quot;P = &quot;</span>, <span class="kw">round</span>(Pi, <span class="dv">3</span>))) <span class="op">+</span></span>
<span id="cb11-78"><a href="#cb11-78"></a><span class="st">      </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb11-79"><a href="#cb11-79"></a><span class="st">      </span><span class="kw">theme</span>(<span class="dt">panel.border =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb11-80"><a href="#cb11-80"></a>            <span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">20</span>),</span>
<span id="cb11-81"><a href="#cb11-81"></a>            <span class="dt">plot.subtitle =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>,</span>
<span id="cb11-82"><a href="#cb11-82"></a>                                         <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">15</span>, <span class="dv">5</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb11-83"><a href="#cb11-83"></a>            <span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">14</span>),</span>
<span id="cb11-84"><a href="#cb11-84"></a>            <span class="dt">axis.text.y =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">14</span>),</span>
<span id="cb11-85"><a href="#cb11-85"></a>            <span class="dt">axis.title.x =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>,</span>
<span id="cb11-86"><a href="#cb11-86"></a>                                        <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb11-87"><a href="#cb11-87"></a>            <span class="dt">axis.title.y =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>,</span>
<span id="cb11-88"><a href="#cb11-88"></a>                                        <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb11-89"><a href="#cb11-89"></a>            <span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb11-90"><a href="#cb11-90"></a>            <span class="dt">legend.justification =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb11-91"><a href="#cb11-91"></a>            <span class="dt">legend.box =</span> <span class="st">&quot;horizontal&quot;</span>,</span>
<span id="cb11-92"><a href="#cb11-92"></a>            <span class="dt">legend.direction =</span> <span class="st">&quot;vertical&quot;</span>,</span>
<span id="cb11-93"><a href="#cb11-93"></a>            <span class="dt">legend.background =</span> <span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="st">&quot;transparent&quot;</span>))</span>
<span id="cb11-94"><a href="#cb11-94"></a>    )</span>
<span id="cb11-95"><a href="#cb11-95"></a></span>
<span id="cb11-96"><a href="#cb11-96"></a>    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(savepath)) {</span>
<span id="cb11-97"><a href="#cb11-97"></a>      <span class="kw">ggsave</span>(<span class="kw">paste0</span>(savepath, <span class="st">&quot;sim_fit_&quot;</span>, i, <span class="st">&quot;.png&quot;</span>))</span>
<span id="cb11-98"><a href="#cb11-98"></a>    }</span>
<span id="cb11-99"><a href="#cb11-99"></a></span>
<span id="cb11-100"><a href="#cb11-100"></a>    st =<span class="st"> </span>st <span class="op">+</span><span class="st"> </span>n</span>
<span id="cb11-101"><a href="#cb11-101"></a>  }</span>
<span id="cb11-102"><a href="#cb11-102"></a>}</span>
<span id="cb11-103"><a href="#cb11-103"></a></span>
<span id="cb11-104"><a href="#cb11-104"></a><span class="kw">plot_sim_fits</span>(sim_fit, K, NTI, V, E, A, D, L, H, Y0, Y1)</span>
<span id="cb11-105"><a href="#cb11-105"></a><span class="co">#&gt; Error in plot_sim_fits(sim_fit, K, NTI, V, E, A, D, L, H, Y0, Y1): object &#39;sim_fit&#39; not found</span></span></code></pre></div>
<p>We can see that everything looks pretty good except for the last fake participant who showed just a very small decrease in median response time. The model finds it difficult to determine where the decrease occurs, and it also confuses <span class="math inline">\(P\)</span> and <span class="math inline">\(D\)</span> a little bit. Just by looking at the plot, I wouldn’t say that the individual had learned the pattern, and I think such a small decrease in response time is a potential problem area for this model. However, I don’t anticipate this trend appearing in the data very frequently, and I also can’t see a better way of handling such data besides saying “the individual may have showed some signs of learning the pattern, but nothing really.” Perhaps in practice this problematic fit could indicate that the individual exhibits atypical learning patters, and maybe that could be useful as a flag to engage with them on a more personal level?</p>
</div>
<div id="fits-real" class="section level2">
<h2>Fitting Real Experimental Data</h2>
<p>Now we’ll use the model to fit data from the actual experiment. We’ll take three individuals who appear to have differing levels of learning: the first will show clear evidence of learning, the second will show no evidence of learning, and the third will show some evidence of learning but will sporadically revert back to their pre-learning median response time. First we’ll load the data from an RDS file because it can be compressed smaller than a CSV, and then we’ll prep everything that we need to run the fitting. Note that as a precaution, we remove all the individuals with fewer than 50 trials (all of the selected individuals have more than 50 trials, but I wanted to keep the code generalizable).</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># load data, will be in the variable &#39;exp1&#39;</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="kw">load</span>(<span class="dt">file =</span> <span class="st">&quot;../exp1.Rds&quot;</span>)</span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="co">#&gt; Warning in readChar(con, 5L, useBytes = TRUE): cannot open compressed file &#39;../</span></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="co">#&gt; exp1.Rds&#39;, probable reason &#39;No such file or directory&#39;</span></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="co">#&gt; Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection</span></span>
<span id="cb12-6"><a href="#cb12-6"></a></span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="co"># use these 3 individuals</span></span>
<span id="cb12-8"><a href="#cb12-8"></a>sub_ids &lt;-<span class="st"> </span><span class="kw">unique</span>(exp1<span class="op">$</span>subject_id)[<span class="kw">c</span>(<span class="dv">78</span>, <span class="dv">7</span>, <span class="dv">10</span>)]</span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="co">#&gt; Error in unique(exp1$subject_id): object &#39;exp1&#39; not found</span></span>
<span id="cb12-10"><a href="#cb12-10"></a></span>
<span id="cb12-11"><a href="#cb12-11"></a><span class="co"># prep for fitting</span></span>
<span id="cb12-12"><a href="#cb12-12"></a>K &lt;-<span class="st"> </span><span class="kw">length</span>(sub_ids)</span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="co">#&gt; Error in eval(expr, envir, enclos): object &#39;sub_ids&#39; not found</span></span>
<span id="cb12-14"><a href="#cb12-14"></a>gs_idx &lt;-<span class="st"> </span><span class="kw">vector</span>()</span>
<span id="cb12-15"><a href="#cb12-15"></a>NTI &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, K)</span>
<span id="cb12-16"><a href="#cb12-16"></a>Y0 &lt;-<span class="st"> </span><span class="kw">vector</span>()</span>
<span id="cb12-17"><a href="#cb12-17"></a>Y1 &lt;-<span class="st"> </span><span class="kw">vector</span>()</span>
<span id="cb12-18"><a href="#cb12-18"></a></span>
<span id="cb12-19"><a href="#cb12-19"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">seq_len</span>(K)) {</span>
<span id="cb12-20"><a href="#cb12-20"></a>  temp &lt;-<span class="st"> </span>exp1[exp1<span class="op">$</span>subject_id <span class="op">==</span><span class="st"> </span>sub_ids[i], ]</span>
<span id="cb12-21"><a href="#cb12-21"></a>  temp0 &lt;-<span class="st"> </span>temp[temp<span class="op">$</span>is_predictable <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, ]<span class="op">$</span>rt<span class="op">/</span><span class="dv">1000</span></span>
<span id="cb12-22"><a href="#cb12-22"></a>  temp1 &lt;-<span class="st"> </span>temp[temp<span class="op">$</span>is_predictable <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, ]<span class="op">$</span>rt<span class="op">/</span><span class="dv">1000</span></span>
<span id="cb12-23"><a href="#cb12-23"></a>  mm &lt;-<span class="st"> </span><span class="kw">min</span>(<span class="kw">length</span>(temp0), <span class="kw">length</span>(temp1))</span>
<span id="cb12-24"><a href="#cb12-24"></a>  <span class="cf">if</span> (mm <span class="op">&gt;=</span><span class="st"> </span><span class="dv">50</span>) {</span>
<span id="cb12-25"><a href="#cb12-25"></a>    NTI[i] &lt;-<span class="st"> </span>mm</span>
<span id="cb12-26"><a href="#cb12-26"></a>    Y0 &lt;-<span class="st"> </span><span class="kw">c</span>(Y0, temp0[<span class="kw">seq_len</span>(mm)])</span>
<span id="cb12-27"><a href="#cb12-27"></a>    Y1 &lt;-<span class="st"> </span><span class="kw">c</span>(Y1, temp1[<span class="kw">seq_len</span>(mm)])</span>
<span id="cb12-28"><a href="#cb12-28"></a>    gs_idx &lt;-<span class="st"> </span><span class="kw">c</span>(gs_idx, i)</span>
<span id="cb12-29"><a href="#cb12-29"></a>  }</span>
<span id="cb12-30"><a href="#cb12-30"></a>}</span>
<span id="cb12-31"><a href="#cb12-31"></a><span class="co">#&gt; Error in eval(expr, envir, enclos): object &#39;exp1&#39; not found</span></span>
<span id="cb12-32"><a href="#cb12-32"></a></span>
<span id="cb12-33"><a href="#cb12-33"></a>good_sub_ids &lt;-<span class="st"> </span>sub_ids[gs_idx]</span>
<span id="cb12-34"><a href="#cb12-34"></a><span class="co">#&gt; Error in eval(expr, envir, enclos): object &#39;sub_ids&#39; not found</span></span>
<span id="cb12-35"><a href="#cb12-35"></a>NTI &lt;-<span class="st"> </span>NTI[NTI <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>]</span>
<span id="cb12-36"><a href="#cb12-36"></a>NK &lt;-<span class="st"> </span><span class="kw">sum</span>(NTI)</span>
<span id="cb12-37"><a href="#cb12-37"></a>K &lt;-<span class="st"> </span><span class="kw">length</span>(good_sub_ids)</span>
<span id="cb12-38"><a href="#cb12-38"></a><span class="co">#&gt; Error in eval(expr, envir, enclos): object &#39;good_sub_ids&#39; not found</span></span></code></pre></div>
<p>Now that everything is ready to go, let’s fit the experimental data. Again, this is a quick and dirty fit with all the precautions from the earlier fit.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>real_fit &lt;-<span class="st"> </span><span class="kw">summary</span>(<span class="kw">sampling</span>(</span>
<span id="cb13-2"><a href="#cb13-2"></a>  exp_log_model,</span>
<span id="cb13-3"><a href="#cb13-3"></a>  <span class="dt">data =</span> <span class="kw">list</span>(<span class="st">&#39;K&#39;</span> =<span class="st"> </span>K, <span class="st">&#39;NTI&#39;</span> =<span class="st"> </span><span class="kw">as.array</span>(NTI), <span class="st">&#39;NK&#39;</span> =<span class="st"> </span>NK, <span class="st">&#39;Y0&#39;</span> =<span class="st"> </span>Y0, <span class="st">&#39;Y1&#39;</span> =<span class="st"> </span>Y1),</span>
<span id="cb13-4"><a href="#cb13-4"></a>  <span class="dt">refresh =</span> <span class="ot">FALSE</span>, <span class="dt">chains =</span> <span class="dv">1</span>, <span class="dt">iter =</span> <span class="dv">500</span>, <span class="dt">seed =</span> <span class="dv">2</span>,</span>
<span id="cb13-5"><a href="#cb13-5"></a>  <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>, <span class="dt">max_treedepth =</span> <span class="dv">10</span>)</span>
<span id="cb13-6"><a href="#cb13-6"></a>))<span class="op">$</span>summary</span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co">#&gt; Error in h(simpleError(msg, call)): error in evaluating the argument &#39;object&#39; in selecting a method for function &#39;summary&#39;: error in evaluating the argument &#39;object&#39; in selecting a method for function &#39;sampling&#39;: object &#39;exp_log_model&#39; not found</span></span>
<span id="cb13-8"><a href="#cb13-8"></a></span>
<span id="cb13-9"><a href="#cb13-9"></a>real_fit[, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>)]</span>
<span id="cb13-10"><a href="#cb13-10"></a><span class="co">#&gt; Error in eval(expr, envir, enclos): object &#39;real_fit&#39; not found</span></span></code></pre></div>
<p>Since we don’t actually have the true value of the model parameters, we’ll just plot how the fit compares to the data. Again, we’ll wrap it all in a function and use that.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>plot_real_fits &lt;-<span class="st"> </span><span class="cf">function</span>(fits, K, NTI, <span class="dt">Y0 =</span> <span class="ot">NA</span>, <span class="dt">Y1 =</span> <span class="ot">NA</span>, <span class="dt">savepath =</span> <span class="ot">NULL</span>) {</span>
<span id="cb14-2"><a href="#cb14-2"></a>  st &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb14-3"><a href="#cb14-3"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</span>
<span id="cb14-4"><a href="#cb14-4"></a>    n &lt;-<span class="st"> </span>NTI[i]</span>
<span id="cb14-5"><a href="#cb14-5"></a>    Vi &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">0</span><span class="op">*</span>K]</span>
<span id="cb14-6"><a href="#cb14-6"></a>    Ei &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">1</span><span class="op">*</span>K]</span>
<span id="cb14-7"><a href="#cb14-7"></a>    Ai &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">2</span><span class="op">*</span>K]</span>
<span id="cb14-8"><a href="#cb14-8"></a>    Pi &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">3</span><span class="op">*</span>K]</span>
<span id="cb14-9"><a href="#cb14-9"></a>    Di &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">4</span><span class="op">*</span>K]</span>
<span id="cb14-10"><a href="#cb14-10"></a>    Li &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">5</span><span class="op">*</span>K]</span>
<span id="cb14-11"><a href="#cb14-11"></a>    Hi &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">8</span><span class="op">*</span>K]</span>
<span id="cb14-12"><a href="#cb14-12"></a>    sigma_2i &lt;-<span class="st"> </span>fits[i<span class="op">+</span><span class="dv">7</span><span class="op">*</span>K]</span>
<span id="cb14-13"><a href="#cb14-13"></a>    sn &lt;-<span class="st"> </span><span class="kw">seq_len</span>(n)</span>
<span id="cb14-14"><a href="#cb14-14"></a></span>
<span id="cb14-15"><a href="#cb14-15"></a>    df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">rep</span>(sn, <span class="dv">2</span>),</span>
<span id="cb14-16"><a href="#cb14-16"></a>                     <span class="dt">data =</span> <span class="kw">c</span>(Y0[(st<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(st<span class="op">+</span>n)], Y1[(st<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(st<span class="op">+</span>n)]),</span>
<span id="cb14-17"><a href="#cb14-17"></a>                     <span class="dt">fit =</span> <span class="kw">c</span>(<span class="kw">exp_log</span>(sn, <span class="dt">V =</span> Vi, <span class="dt">E =</span> Ei, <span class="dt">A =</span> Ai,</span>
<span id="cb14-18"><a href="#cb14-18"></a>                                     <span class="dt">D =</span> <span class="dv">0</span>, <span class="dt">L =</span> Li, <span class="dt">H =</span> Hi),</span>
<span id="cb14-19"><a href="#cb14-19"></a>                             <span class="kw">exp_log</span>(sn, <span class="dt">V =</span> Vi, <span class="dt">E =</span> Ei, <span class="dt">A =</span> Ai,</span>
<span id="cb14-20"><a href="#cb14-20"></a>                                     <span class="dt">D =</span> Pi <span class="op">*</span><span class="st"> </span>Di, <span class="dt">L =</span> Li, <span class="dt">H =</span> Hi)),</span>
<span id="cb14-21"><a href="#cb14-21"></a>                     <span class="dt">data_label =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n), <span class="kw">rep</span>(<span class="dv">2</span>,n)),</span>
<span id="cb14-22"><a href="#cb14-22"></a>                     <span class="dt">fit_label =</span> <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">3</span>,n), <span class="kw">rep</span>(<span class="dv">4</span>,n)))</span>
<span id="cb14-23"><a href="#cb14-23"></a>    df<span class="op">$</span>fit_min &lt;-<span class="st"> </span>df<span class="op">$</span>fit <span class="op">-</span><span class="st"> </span>df<span class="op">$</span>fit<span class="op">*</span><span class="kw">sqrt</span>(sigma_2i<span class="op">*</span>(sigma_2i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span>
<span id="cb14-24"><a href="#cb14-24"></a>    df<span class="op">$</span>fit_max &lt;-<span class="st"> </span>df<span class="op">$</span>fit <span class="op">+</span><span class="st"> </span>df<span class="op">$</span>fit<span class="op">*</span><span class="kw">sqrt</span>(sigma_2i<span class="op">*</span>(sigma_2i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</span>
<span id="cb14-25"><a href="#cb14-25"></a></span>
<span id="cb14-26"><a href="#cb14-26"></a>    <span class="co"># factor levels key (since R makes it alphabetical)</span></span>
<span id="cb14-27"><a href="#cb14-27"></a>      <span class="co"># 1: Non-Learned Data/Truth</span></span>
<span id="cb14-28"><a href="#cb14-28"></a>      <span class="co"># 2: Learned Data/Truth</span></span>
<span id="cb14-29"><a href="#cb14-29"></a>      <span class="co"># 3: Non-Learned Fit</span></span>
<span id="cb14-30"><a href="#cb14-30"></a>      <span class="co"># 4: Learned Fit</span></span>
<span id="cb14-31"><a href="#cb14-31"></a></span>
<span id="cb14-32"><a href="#cb14-32"></a>    <span class="kw">print</span>(<span class="kw">ggplot</span>(df) <span class="op">+</span></span>
<span id="cb14-33"><a href="#cb14-33"></a><span class="st">      </span><span class="kw">geom_ribbon</span>(<span class="dt">linetype=</span><span class="st">&quot;blank&quot;</span>, <span class="dt">alpha=</span><span class="fl">0.25</span>,</span>
<span id="cb14-34"><a href="#cb14-34"></a>                  <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">ymin =</span> fit_min, <span class="dt">ymax =</span> fit_max,</span>
<span id="cb14-35"><a href="#cb14-35"></a>                      <span class="dt">fill =</span> <span class="kw">factor</span>(fit_label, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>)))) <span class="op">+</span></span>
<span id="cb14-36"><a href="#cb14-36"></a><span class="st">      </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="fl">0.75</span>, <span class="dt">shape =</span> <span class="dv">16</span>, <span class="dt">size =</span> <span class="fl">1.8</span>,</span>
<span id="cb14-37"><a href="#cb14-37"></a>                 <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> data,</span>
<span id="cb14-38"><a href="#cb14-38"></a>                     <span class="dt">color =</span> <span class="kw">factor</span>(data_label, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)))) <span class="op">+</span></span>
<span id="cb14-39"><a href="#cb14-39"></a><span class="st">      </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.25</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>,</span>
<span id="cb14-40"><a href="#cb14-40"></a>                <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> fit,</span>
<span id="cb14-41"><a href="#cb14-41"></a>                    <span class="dt">color =</span> <span class="kw">factor</span>(fit_label, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>)))) <span class="op">+</span></span>
<span id="cb14-42"><a href="#cb14-42"></a><span class="st">      </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;#999999&quot;</span>, <span class="st">&quot;#000000&quot;</span>,</span>
<span id="cb14-43"><a href="#cb14-43"></a>                                    <span class="st">&quot;#98c1ff&quot;</span>, <span class="st">&quot;#ff4f4f&quot;</span>),</span>
<span id="cb14-44"><a href="#cb14-44"></a>                         <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;Non-Learned Data&quot;</span>, <span class="st">&quot;Learned Data&quot;</span>,</span>
<span id="cb14-45"><a href="#cb14-45"></a>                                    <span class="st">&quot;Non-Learned Fit&quot;</span>, <span class="st">&quot;Learned Fit&quot;</span>),</span>
<span id="cb14-46"><a href="#cb14-46"></a>                         <span class="dt">name =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb14-47"><a href="#cb14-47"></a><span class="st">      </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;#98c1ff&quot;</span>, <span class="st">&quot;#ff4f4f&quot;</span>),</span>
<span id="cb14-48"><a href="#cb14-48"></a>                        <span class="dt">guide =</span> <span class="ot">FALSE</span>) <span class="op">+</span></span>
<span id="cb14-49"><a href="#cb14-49"></a><span class="st">      </span><span class="kw">guides</span>(<span class="dt">color =</span> <span class="kw">guide_legend</span>(<span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">size =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="fl">1.25</span>, <span class="fl">1.25</span>),</span>
<span id="cb14-50"><a href="#cb14-50"></a>                                                      <span class="dt">shape =</span> <span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">16</span>, <span class="ot">NA</span>, <span class="ot">NA</span>),</span>
<span id="cb14-51"><a href="#cb14-51"></a>                                                      <span class="dt">lty =</span> <span class="kw">c</span>(<span class="ot">NA</span>, <span class="ot">NA</span>, <span class="dv">1</span>, <span class="dv">1</span>)))) <span class="op">+</span></span>
<span id="cb14-52"><a href="#cb14-52"></a><span class="st">      </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Experimental Data Fit&quot;</span>,</span>
<span id="cb14-53"><a href="#cb14-53"></a>           <span class="dt">x =</span> <span class="st">&quot;Trial Number&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Response Time (sec)&quot;</span>,</span>
<span id="cb14-54"><a href="#cb14-54"></a>           <span class="dt">subtitle =</span> <span class="kw">paste0</span>(<span class="st">&quot;\u00B1 1 standard deviation</span><span class="ch">\n</span><span class="st">&quot;</span>,</span>
<span id="cb14-55"><a href="#cb14-55"></a>                             <span class="st">&quot;Subject &quot;</span>, i, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>,</span>
<span id="cb14-56"><a href="#cb14-56"></a>                             <span class="st">&quot;P = &quot;</span>, <span class="kw">round</span>(Pi, <span class="dv">3</span>))) <span class="op">+</span></span>
<span id="cb14-57"><a href="#cb14-57"></a><span class="st">      </span><span class="kw">theme_bw</span>() <span class="op">+</span></span>
<span id="cb14-58"><a href="#cb14-58"></a><span class="st">      </span><span class="kw">theme</span>(<span class="dt">panel.border =</span> <span class="kw">element_blank</span>(),</span>
<span id="cb14-59"><a href="#cb14-59"></a>            <span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">20</span>),</span>
<span id="cb14-60"><a href="#cb14-60"></a>            <span class="dt">plot.subtitle =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>,</span>
<span id="cb14-61"><a href="#cb14-61"></a>                                         <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">15</span>, <span class="dv">5</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb14-62"><a href="#cb14-62"></a>            <span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">14</span>),</span>
<span id="cb14-63"><a href="#cb14-63"></a>            <span class="dt">axis.text.y =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">14</span>),</span>
<span id="cb14-64"><a href="#cb14-64"></a>            <span class="dt">axis.title.x =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>,</span>
<span id="cb14-65"><a href="#cb14-65"></a>                                        <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb14-66"><a href="#cb14-66"></a>            <span class="dt">axis.title.y =</span> <span class="kw">element_text</span>(<span class="dt">size =</span> <span class="dv">16</span>,</span>
<span id="cb14-67"><a href="#cb14-67"></a>                                        <span class="dt">margin =</span> <span class="kw">margin</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="st">&quot;pt&quot;</span>)),</span>
<span id="cb14-68"><a href="#cb14-68"></a>            <span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb14-69"><a href="#cb14-69"></a>            <span class="dt">legend.justification =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>),</span>
<span id="cb14-70"><a href="#cb14-70"></a>            <span class="dt">legend.box =</span> <span class="st">&quot;horizontal&quot;</span>,</span>
<span id="cb14-71"><a href="#cb14-71"></a>            <span class="dt">legend.direction =</span> <span class="st">&quot;vertical&quot;</span>,</span>
<span id="cb14-72"><a href="#cb14-72"></a>            <span class="dt">legend.background =</span> <span class="kw">element_rect</span>(<span class="dt">fill =</span> <span class="st">&quot;transparent&quot;</span>))</span>
<span id="cb14-73"><a href="#cb14-73"></a>    )</span>
<span id="cb14-74"><a href="#cb14-74"></a></span>
<span id="cb14-75"><a href="#cb14-75"></a>    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(savepath)) {</span>
<span id="cb14-76"><a href="#cb14-76"></a>      <span class="kw">ggsave</span>(<span class="kw">paste0</span>(savepath, <span class="st">&quot;real_fit_&quot;</span>, i, <span class="st">&quot;.png&quot;</span>))</span>
<span id="cb14-77"><a href="#cb14-77"></a>    }</span>
<span id="cb14-78"><a href="#cb14-78"></a></span>
<span id="cb14-79"><a href="#cb14-79"></a>    st =<span class="st"> </span>st <span class="op">+</span><span class="st"> </span>n</span>
<span id="cb14-80"><a href="#cb14-80"></a>  }</span>
<span id="cb14-81"><a href="#cb14-81"></a>}</span>
<span id="cb14-82"><a href="#cb14-82"></a></span>
<span id="cb14-83"><a href="#cb14-83"></a><span class="kw">plot_real_fits</span>(real_fit, K, NTI, Y0, Y1)</span>
<span id="cb14-84"><a href="#cb14-84"></a><span class="co">#&gt; Error in plot_real_fits(real_fit, K, NTI, Y0, Y1): object &#39;real_fit&#39; not found</span></span></code></pre></div>
<p>We see similar results compared to fitting the simulated data. The model identifies both the individual who showed evidence of learning the pattern, and the individual who showed no evidence of learning the pattern. The trouble again arises when the data show weird inconsistencies in response times. The third individual showed maybe some signs of learning the pattern fairly early on in the experiment, but the variance seems to increase upon adjusting to the task. The model struggles to fit this, and maybe we can treat this as an indication of a poorly performing participant who needs closer examination? I think the only way to fit a model to this type of behavior is to incorporate some kind of stochasticity into the model to allow for sporadic reversions back to the pre-learning median response time. The tricky thing is that we don’t want to overfit the data and end up fitting a bunch of noise because the participants were distracted while completing the task. We also need to be careful about the model potentially confusing regular variance in the response times for this newfangled stochasticity. I suppose we could change the <span class="math inline">\(\hat{\sigma^2}\)</span> parameter to change upon learning, and allow that to increase so that the variance increases. I don’t know if trying to fit bored people is particularly fruitful though, and if it’s just the occassional response time that reverts back to the pre-learning median then I think the stochastic part should handle that well enough.</p>
</div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references hanging-indent">
<div id="ref-dablander2019bayesian">
<p>Dablander, Fabian. 2019. “Bayesian Modeling Using Stan: A Case Study.” May 30, 2019. <a href="https://fabiandablander.com/r/Law-of-Practice.html">https://fabiandablander.com/r/Law-of-Practice.html</a>.</p>
</div>
<div id="ref-de2016dynamic">
<p>Leeuw, Joshua R de. 2016. “Dynamic Constraints in Statistical Learning.” PhD thesis, Indiana University.</p>
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
